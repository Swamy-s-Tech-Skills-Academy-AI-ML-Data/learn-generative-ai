{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e607c8",
   "metadata": {},
   "source": [
    "# Chapter 01: Introduction to Generative AI Concepts\n",
    "\n",
    "üöÄ **Learn Generative AI ‚Äî Code-First, Book-Free**\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **What is Generative AI** and how it differs from traditional machine learning\n",
    "2. **Key terminology** and concepts in the GenAI landscape\n",
    "3. **The evolution** from RNNs ‚Üí Transformers ‚Üí Large Language Models\n",
    "4. **Mathematical foundations** behind attention mechanisms (simplified)\n",
    "5. **Practical applications** and real-world use cases\n",
    "6. **The path ahead** - from foundations to AI agents\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites Checklist\n",
    "\n",
    "- ‚úÖ Python 3.8+ installed\n",
    "- ‚úÖ Virtual environment set up (`.venv`)\n",
    "- ‚úÖ Basic packages installed (`numpy`, `matplotlib`, `pandas`)\n",
    "- ‚úÖ Curiosity and willingness to experiment!\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Remember**: This is a **code-first** approach. We'll understand concepts by implementing and visualizing them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeec629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Print environment info\n",
    "print(\"üéâ Environment Setup Complete!\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python Libraries:\")\n",
    "print(f\"   - NumPy: {np.__version__}\")\n",
    "print(f\"   - Pandas: {pd.__version__}\")\n",
    "print(f\"   - Matplotlib: Ready for visualizations!\")\n",
    "print(f\"\\nüöÄ Ready to learn Generative AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf796c06",
   "metadata": {},
   "source": [
    "## ü§ñ Part 1: What is Generative AI?\n",
    "\n",
    "### Traditional AI vs Generative AI\n",
    "\n",
    "**Traditional AI (Discriminative Models):**\n",
    "\n",
    "- Analyzes and classifies existing data\n",
    "- Answers: \"What is this?\" or \"Which category?\"\n",
    "- Examples: Image classification, spam detection, sentiment analysis\n",
    "\n",
    "**Generative AI (Generative Models):**\n",
    "\n",
    "- Creates new data that resembles training data\n",
    "- Answers: \"What would new data look like?\"\n",
    "- Examples: Text generation, image creation, code synthesis\n",
    "\n",
    "### üéØ Key Insight\n",
    "\n",
    "> **Generative AI doesn't just understand data ‚Äî it creates new data!**\n",
    "\n",
    "Let's visualize this difference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Traditional AI vs Generative AI\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Traditional AI (Classification)\n",
    "np.random.seed(42)\n",
    "class_a = np.random.normal(2, 0.5, (50, 2))\n",
    "class_b = np.random.normal(4, 0.5, (50, 2))\n",
    "\n",
    "ax1.scatter(class_a[:, 0], class_a[:, 1], c='blue',\n",
    "            alpha=0.7, label='Class A', s=60)\n",
    "ax1.scatter(class_b[:, 0], class_b[:, 1], c='red',\n",
    "            alpha=0.7, label='Class B', s=60)\n",
    "\n",
    "# Decision boundary (simplified)\n",
    "x_line = np.linspace(1, 5, 100)\n",
    "y_line = 3 - 0.5 * x_line + 3  # Simple linear boundary\n",
    "ax1.plot(x_line, y_line, 'black', linewidth=2, linestyle='--', alpha=0.8)\n",
    "\n",
    "ax1.set_title('Traditional AI\\n(Discriminative)',\n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.legend()\n",
    "ax1.text(1.5, 5, 'Classifies\\nexisting data', fontsize=12,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "\n",
    "# Generative AI (Generation)\n",
    "# Show data generation process\n",
    "original_data = np.random.normal(3, 0.8, (30, 2))\n",
    "generated_data = np.random.normal(3, 0.8, (20, 2))  # \"Generated\" data\n",
    "\n",
    "ax2.scatter(original_data[:, 0], original_data[:, 1], c='green', alpha=0.7,\n",
    "            label='Training Data', s=60, marker='o')\n",
    "ax2.scatter(generated_data[:, 0], generated_data[:, 1], c='orange', alpha=0.9,\n",
    "            label='Generated Data', s=80, marker='*')\n",
    "\n",
    "ax2.set_title('Generative AI\\n(Generative)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.legend()\n",
    "ax2.text(4.5, 1, 'Creates\\nnew data', fontsize=12,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Key Difference:\")\n",
    "print(\"Traditional AI: Input ‚Üí Classification/Prediction\")\n",
    "print(\"Generative AI:   Input ‚Üí New Content Creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb14f6",
   "metadata": {},
   "source": [
    "## üìö Part 2: Essential Terminology\n",
    "\n",
    "### üéØ Core Concepts You Must Know\n",
    "\n",
    "| Term            | Definition                           | Example                                  |\n",
    "| --------------- | ------------------------------------ | ---------------------------------------- |\n",
    "| **LLM**         | Large Language Model                 | GPT-4, Claude, Gemini                    |\n",
    "| **Transformer** | Neural architecture using attention  | The backbone of modern LLMs              |\n",
    "| **Attention**   | Mechanism to focus on relevant parts | Looking at specific words in a sentence  |\n",
    "| **Token**       | Smallest unit of text processing     | Words or subwords                        |\n",
    "| **Prompt**      | Input text to guide AI generation    | \"Write a poem about...\"                  |\n",
    "| **Fine-tuning** | Adapting pre-trained models          | Training ChatGPT to be helpful           |\n",
    "| **RAG**         | Retrieval-Augmented Generation       | Combining knowledge base with LLM        |\n",
    "| **Agent**       | AI system that can take actions      | AI that can use tools and make decisions |\n",
    "\n",
    "### üîÑ The Generation Process\n",
    "\n",
    "1. **Input** (Prompt) ‚Üí 2. **Processing** (Model) ‚Üí 3. **Output** (Generated Content)\n",
    "\n",
    "Let's see how this works with a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tokenization Example (conceptual)\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple tokenization for demonstration\"\"\"\n",
    "    # Split by spaces and punctuation (very basic)\n",
    "    import re\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def visualize_tokenization(text):\n",
    "    \"\"\"Visualize how text gets broken into tokens\"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "\n",
    "    print(f\"üìù Original text: '{text}'\")\n",
    "    print(f\"üîç Tokens: {tokens}\")\n",
    "    print(f\"üìä Number of tokens: {len(tokens)}\")\n",
    "\n",
    "    # Show token-by-token breakdown\n",
    "    print(\"\\nüéØ Token breakdown:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"  {i+1:2d}: '{token}'\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Hello, world! How are you today?\"\n",
    "tokens = visualize_tokenization(sample_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üí° Key Insight:\")\n",
    "print(\"   - Models don't see 'words' - they see 'tokens'\")\n",
    "print(\"   - Each token gets converted to numbers (embeddings)\")\n",
    "print(\"   - This is how AI 'understands' text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54fdc6",
   "metadata": {},
   "source": [
    "## üöÄ Part 3: The Evolution - From RNNs to Transformers to LLMs\n",
    "\n",
    "### üìà The Journey of Language Models\n",
    "\n",
    "```\n",
    "1990s-2000s: Statistical Models (N-grams)\n",
    "     ‚Üì\n",
    "2010s: Recurrent Neural Networks (RNNs/LSTMs)\n",
    "     ‚Üì\n",
    "2017: Transformers (\"Attention Is All You Need\")\n",
    "     ‚Üì\n",
    "2018-2019: BERT, GPT-1, GPT-2\n",
    "     ‚Üì\n",
    "2020: GPT-3 (175B parameters)\n",
    "     ‚Üì\n",
    "2022: ChatGPT & GPT-4\n",
    "     ‚Üì\n",
    "2023-2024: Claude, Gemini, and the LLM explosion\n",
    "     ‚Üì\n",
    "2024+: AI Agents & Agentic AI\n",
    "```\n",
    "\n",
    "### üîÑ Why Transformers Changed Everything\n",
    "\n",
    "**Problems with RNNs:**\n",
    "\n",
    "- ‚ùå Sequential processing (slow)\n",
    "- ‚ùå Vanishing gradients (long sequences)\n",
    "- ‚ùå Hard to parallelize\n",
    "\n",
    "**Transformers Solution:**\n",
    "\n",
    "- ‚úÖ Parallel processing (fast)\n",
    "- ‚úÖ Attention mechanism (long-range dependencies)\n",
    "- ‚úÖ Scalable architecture\n",
    "\n",
    "Let's visualize the difference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: RNN vs Transformer Processing\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# RNN Sequential Processing\n",
    "sequence_length = 6\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# RNN processing (sequential)\n",
    "ax1.set_title(\"RNN: Sequential Processing\",\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "for i in range(sequence_length):\n",
    "    # Word boxes\n",
    "    rect = plt.Rectangle((i*2, 0), 1.5, 0.8, facecolor='lightblue',\n",
    "                         edgecolor='blue', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(i*2 + 0.75, 0.4, words[i],\n",
    "             ha='center', va='center', fontweight='bold')\n",
    "\n",
    "    # Hidden state boxes\n",
    "    rect = plt.Rectangle((i*2, 1.5), 1.5, 0.8, facecolor='lightcoral',\n",
    "                         edgecolor='red', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(i*2 + 0.75, 1.9, f'h{i+1}',\n",
    "             ha='center', va='center', fontweight='bold')\n",
    "\n",
    "    # Arrows\n",
    "    if i < sequence_length - 1:\n",
    "        ax1.arrow(i*2 + 1.5, 1.9, 0.4, 0, head_width=0.1, head_length=0.1,\n",
    "                  fc='red', ec='red')\n",
    "\n",
    "    # Vertical connections\n",
    "    ax1.arrow(i*2 + 0.75, 0.8, 0, 0.6, head_width=0.1, head_length=0.05,\n",
    "              fc='black', ec='black')\n",
    "\n",
    "ax1.set_xlim(-0.5, 12)\n",
    "ax1.set_ylim(-0.5, 3)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.text(6, -0.3, \"Sequential: Each word processed one after another\",\n",
    "         ha='center', fontsize=12, style='italic')\n",
    "\n",
    "# Transformer Parallel Processing\n",
    "ax2.set_title(\"Transformer: Parallel Processing with Attention\",\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# All words processed simultaneously\n",
    "for i in range(sequence_length):\n",
    "    # Word boxes\n",
    "    rect = plt.Rectangle((i*2, 0), 1.5, 0.8, facecolor='lightgreen',\n",
    "                         edgecolor='green', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(i*2 + 0.75, 0.4, words[i],\n",
    "             ha='center', va='center', fontweight='bold')\n",
    "\n",
    "    # Attention layer\n",
    "    rect = plt.Rectangle((i*2, 1.5), 1.5, 0.8, facecolor='lightyellow',\n",
    "                         edgecolor='orange', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(i*2 + 0.75, 1.9, 'Attn', ha='center',\n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "# Attention connections (showing some connections)\n",
    "# \"the\" connects to \"the\", \"cat\" to \"mat\", etc.\n",
    "attention_pairs = [(0, 4), (1, 5), (2, 4), (3, 5)]\n",
    "for i, j in attention_pairs:\n",
    "    ax2.plot([i*2 + 0.75, j*2 + 0.75], [1.5, 1.5],\n",
    "             'orange', linewidth=3, alpha=0.7)\n",
    "    ax2.plot([i*2 + 0.75, j*2 + 0.75], [1.45, 1.55],\n",
    "             'orange', linewidth=2, alpha=0.5)\n",
    "\n",
    "# Vertical connections\n",
    "for i in range(sequence_length):\n",
    "    ax2.arrow(i*2 + 0.75, 0.8, 0, 0.6, head_width=0.1, head_length=0.05,\n",
    "              fc='black', ec='black')\n",
    "\n",
    "ax2.set_xlim(-0.5, 12)\n",
    "ax2.set_ylim(-0.5, 3)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.text(6, -0.3, \"Parallel: All words processed simultaneously with attention connections\",\n",
    "         ha='center', fontsize=12, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üöÄ Key Advantages of Transformers:\")\n",
    "print(\"   ‚úÖ Parallel processing ‚Üí Faster training\")\n",
    "print(\"   ‚úÖ Attention mechanism ‚Üí Better long-range dependencies\")\n",
    "print(\"   ‚úÖ Scalable ‚Üí Can build very large models\")\n",
    "print(\"   ‚úÖ Transfer learning ‚Üí Pre-train once, fine-tune for many tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0078d9",
   "metadata": {},
   "source": [
    "## üîç Part 4: Understanding Attention (Simplified)\n",
    "\n",
    "### üß† What is Attention?\n",
    "\n",
    "**Human Analogy:** When you read this sentence, you don't give equal attention to every word - you focus on the important ones.\n",
    "\n",
    "**In AI:** Attention allows the model to focus on relevant parts of the input when generating each output token.\n",
    "\n",
    "### üìù Example: Translation Task\n",
    "\n",
    "**Input:** \"The cat sat on the mat\"  \n",
    "**Output:** \"Le chat s'est assis sur le tapis\"\n",
    "\n",
    "When generating \"chat\" (cat), the model should pay attention to \"cat\" in the input, not \"the\" or \"mat\".\n",
    "\n",
    "### üéØ Attention Formula (Simplified)\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q¬∑K^T / ‚àöd) ¬∑ V\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Q** = Query (what we're looking for)\n",
    "- **K** = Key (what we're looking in)\n",
    "- **V** = Value (what we want to retrieve)\n",
    "\n",
    "Don't worry about the math yet - we'll implement this in Week 2! For now, let's visualize the concept:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07067d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Visualization\n",
    "def visualize_attention(input_words, output_word, attention_weights):\n",
    "    \"\"\"Visualize attention weights between input and output\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create attention heatmap\n",
    "    attention_matrix = np.array(attention_weights).reshape(1, -1)\n",
    "\n",
    "    im = ax.imshow(attention_matrix, cmap='Blues', aspect='auto')\n",
    "\n",
    "    # Set labels\n",
    "    ax.set_xticks(range(len(input_words)))\n",
    "    ax.set_xticklabels(input_words, fontsize=12)\n",
    "    ax.set_yticks([0])\n",
    "    ax.set_yticklabels([output_word], fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Add attention weight text\n",
    "    for i, weight in enumerate(attention_weights):\n",
    "        ax.text(i, 0, f'{weight:.2f}', ha='center', va='center',\n",
    "                fontweight='bold', fontsize=10,\n",
    "                color='white' if weight > 0.5 else 'black')\n",
    "\n",
    "    ax.set_title(\n",
    "        f'Attention: Generating \"{output_word}\"', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Input Words', fontsize=14)\n",
    "    ax.set_ylabel('Output Word', fontsize=14)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Attention Weight', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example 1: Translating \"cat\"\n",
    "input_sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "output_word = \"chat\"  # French for \"cat\"\n",
    "# High attention on \"cat\", low on others\n",
    "attention_weights = [0.1, 0.8, 0.05, 0.02, 0.02, 0.01]\n",
    "\n",
    "print(\"üåç Translation Example:\")\n",
    "print(f\"Input: {' '.join(input_sentence)}\")\n",
    "print(f\"Generating: '{output_word}' (French for 'cat')\")\n",
    "print(\"Notice how attention focuses on 'cat'!\")\n",
    "print()\n",
    "\n",
    "visualize_attention(input_sentence, output_word, attention_weights)\n",
    "\n",
    "# Example 2: More complex attention pattern\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ Another Example:\")\n",
    "output_word2 = \"tapis\"  # French for \"mat\"\n",
    "attention_weights2 = [0.02, 0.05, 0.1, 0.15, 0.08, 0.6]  # Focus on \"mat\"\n",
    "\n",
    "print(f\"Generating: '{output_word2}' (French for 'mat')\")\n",
    "print(\"Notice how attention shifts to 'mat'!\")\n",
    "print()\n",
    "\n",
    "visualize_attention(input_sentence, output_word2, attention_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729fe4f",
   "metadata": {},
   "source": [
    "## üåü Part 5: Real-World Applications & The AI Landscape\n",
    "\n",
    "### üéØ Current Applications of Generative AI\n",
    "\n",
    "| Domain         | Application                   | Examples                             |\n",
    "| -------------- | ----------------------------- | ------------------------------------ |\n",
    "| **Text**       | Content writing, coding, chat | ChatGPT, GitHub Copilot, Claude      |\n",
    "| **Images**     | Art generation, editing       | DALL-E, Midjourney, Stable Diffusion |\n",
    "| **Code**       | Programming assistance        | GitHub Copilot, CodeT5, Replit       |\n",
    "| **Audio**      | Music, speech synthesis       | Eleven Labs, Mubert                  |\n",
    "| **Video**      | Video generation              | RunwayML, Pika Labs                  |\n",
    "| **Multimodal** | Vision + Language             | GPT-4V, Claude 3                     |\n",
    "\n",
    "### üöÄ The Path to AI Agents\n",
    "\n",
    "```\n",
    "Basic GenAI ‚Üí Advanced GenAI ‚Üí AI Agents ‚Üí Agentic AI\n",
    "    ‚Üì              ‚Üì              ‚Üì           ‚Üì\n",
    "Text Gen    ‚Üí  Multi-modal  ‚Üí  Tool Use  ‚Üí  Autonomous\n",
    "Chatbots    ‚Üí  Understanding ‚Üí Planning   ‚Üí  Systems\n",
    "Simple      ‚Üí  Complex       ‚Üí Memory     ‚Üí  Goal-oriented\n",
    "```\n",
    "\n",
    "### üé≠ From Simple to Sophisticated\n",
    "\n",
    "Let's visualize the progression from simple text generation to AI agents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccecbd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# AI Capability Progression Visualization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig, ax = \u001b[43mplt\u001b[49m.subplots(figsize=(\u001b[32m14\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define the progression stages\u001b[39;00m\n\u001b[32m      5\u001b[39m stages = [\u001b[33m\"\u001b[39m\u001b[33mBasic\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mText Gen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAdvanced\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLLMs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mTool-Using\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAgents\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAutonomous\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAI Systems\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# AI Capability Progression Visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Define the progression stages\n",
    "stages = [\"Basic\\nText Gen\", \"Advanced\\nLLMs\",\n",
    "          \"Tool-Using\\nAgents\", \"Autonomous\\nAI Systems\"]\n",
    "capabilities = [\n",
    "    [\"Simple completion\", \"Pattern matching\", \"Basic responses\"],\n",
    "    [\"Complex reasoning\", \"Multi-turn chat\", \"Context awareness\"],\n",
    "    [\"Tool integration\", \"Planning\", \"Memory\"],\n",
    "    [\"Goal-oriented\", \"Self-directed\", \"Multi-agent collaboration\"]\n",
    "]\n",
    "\n",
    "colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "positions = [1, 3, 5, 7]\n",
    "\n",
    "# Create progression chart\n",
    "for i, (stage, caps, color, pos) in enumerate(zip(stages, capabilities, colors, positions)):\n",
    "    # Main stage box\n",
    "    rect = plt.Rectangle((pos-0.8, 4), 1.6, 2, facecolor=color,\n",
    "                         edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(pos, 5, stage, ha='center', va='center',\n",
    "            fontweight='bold', fontsize=11)\n",
    "\n",
    "    # Capability boxes\n",
    "    for j, cap in enumerate(caps):\n",
    "        rect = plt.Rectangle((pos-0.8, 3-j*0.8), 1.6, 0.7, facecolor='white',\n",
    "                             edgecolor=color, linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(pos, 2.65-j*0.8, cap, ha='center', va='center', fontsize=9)\n",
    "\n",
    "    # Arrows between stages\n",
    "    if i < len(stages) - 1:\n",
    "        ax.arrow(pos + 0.8, 5, 1.4, 0, head_width=0.2, head_length=0.2,\n",
    "                 fc='darkblue', ec='darkblue', linewidth=2)\n",
    "\n",
    "# Add timeline\n",
    "ax.text(1, 7, \"2020-2021\", ha='center', fontsize=10, style='italic')\n",
    "ax.text(3, 7, \"2022-2023\", ha='center', fontsize=10, style='italic')\n",
    "ax.text(5, 7, \"2023-2024\", ha='center', fontsize=10, style='italic')\n",
    "ax.text(7, 7, \"2024+\", ha='center', fontsize=10, style='italic')\n",
    "\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.set_title(\"The Evolution of AI Capabilities\",\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ We Are Here: Transitioning from Advanced LLMs to AI Agents\")\n",
    "print(\"\\nüìö Your Learning Journey:\")\n",
    "print(\"   Week 1-2: Understanding the foundations (Transformers)\")\n",
    "print(\"   Week 3-4: Building and using LLMs\")\n",
    "print(\"   Week 5+:  Creating AI Agents that can act autonomously\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c6626",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Part 6: Your Learning Path Ahead\n",
    "\n",
    "### üìÖ 9-Week Comprehensive Learning Journey (5 Days Per Week)\n",
    "\n",
    "#### **Week 1: Mathematical Foundations & Attention Basics** üßÆ\n",
    "\n",
    "- **Day 1:** Linear algebra foundations (vectors, matrices, dot products)\n",
    "- **Day 2:** Introduction to attention mechanisms & intuitive understanding\n",
    "- **Day 3:** Scaled dot-product attention - theory and visualization\n",
    "- **Day 4:** Single-head attention implementation from scratch\n",
    "- **Day 5:** Practice session: Building attention visualizations\n",
    "\n",
    "#### **Week 2: Multi-Head Attention & Positional Encoding** üîÑ\n",
    "\n",
    "- **Day 1:** Multi-head attention concept and implementation\n",
    "- **Day 2:** Positional encoding - why and how it works\n",
    "- **Day 3:** Sinusoidal vs learned positional embeddings\n",
    "- **Day 4:** Combining attention with positional information\n",
    "- **Day 5:** Complete attention block with layer normalization\n",
    "\n",
    "#### **Week 3: Transformer Encoder Architecture** üèóÔ∏è\n",
    "\n",
    "- **Day 1:** Feed-forward networks and residual connections\n",
    "- **Day 2:** Layer normalization and its importance\n",
    "- **Day 3:** Complete transformer encoder block implementation\n",
    "- **Day 4:** Stacking multiple encoder layers\n",
    "- **Day 5:** BERT-style encoder architecture deep dive\n",
    "\n",
    "#### **Week 4: Transformer Decoder Architecture** üéØ\n",
    "\n",
    "- **Day 1:** Masked self-attention for autoregressive generation\n",
    "- **Day 2:** Cross-attention between encoder and decoder\n",
    "- **Day 3:** Complete transformer decoder implementation\n",
    "- **Day 4:** GPT-style decoder-only architecture\n",
    "- **Day 5:** Comparing encoder vs decoder vs encoder-decoder models\n",
    "\n",
    "#### **Week 5: Complete Transformer Implementation** ‚öôÔ∏è\n",
    "\n",
    "- **Day 1:** Mini-transformer from scratch - architecture design\n",
    "- **Day 2:** Training loop and loss functions\n",
    "- **Day 3:** Tokenization and data preprocessing\n",
    "- **Day 4:** Testing and debugging your transformer\n",
    "- **Day 5:** Performance optimization and analysis\n",
    "\n",
    "#### **Week 6: Pre-training and Modern LLM Architectures** üöÄ\n",
    "\n",
    "- **Day 1:** Pre-training strategies and objectives\n",
    "- **Day 2:** Data preparation and cleaning pipelines\n",
    "- **Day 3:** Modern architectures: GPT-4, Claude, Gemini differences\n",
    "- **Day 4:** Scaling laws and parameter efficiency\n",
    "- **Day 5:** Understanding emergent capabilities\n",
    "\n",
    "#### **Week 7: Fine-tuning and Adaptation Techniques** üé®\n",
    "\n",
    "- **Day 1:** Full fine-tuning vs parameter-efficient methods\n",
    "- **Day 2:** LoRA (Low-Rank Adaptation) implementation\n",
    "- **Day 3:** QLoRA and other efficient fine-tuning methods\n",
    "- **Day 4:** Instruction tuning and dataset preparation\n",
    "- **Day 5:** Hands-on project: Fine-tune your own model\n",
    "\n",
    "#### **Week 8: Advanced Training and RLHF** üß†\n",
    "\n",
    "- **Day 1:** Reinforcement Learning from Human Feedback (RLHF)\n",
    "- **Day 2:** Constitutional AI and safety considerations\n",
    "- **Day 3:** Advanced prompting and chain-of-thought reasoning\n",
    "- **Day 4:** Model evaluation and benchmarking\n",
    "- **Day 5:** Bias detection and mitigation strategies\n",
    "\n",
    "#### **Week 9: Production and Applications** üåü\n",
    "\n",
    "- **Day 1:** RAG (Retrieval-Augmented Generation) systems\n",
    "- **Day 2:** Vector databases and semantic search\n",
    "- **Day 3:** LLM APIs, integration patterns, and optimization\n",
    "- **Day 4:** Production deployment, monitoring, and scaling\n",
    "- **Day 5:** Complete capstone project: End-to-end LLM application\n",
    "\n",
    "### üöÄ **Beyond Week 9: AI Agents & Advanced Topics**\n",
    "\n",
    "#### **Week 10+: AI Agents and Agentic AI**\n",
    "\n",
    "- Multi-agent systems and autonomous AI\n",
    "- Tool use, function calling, and external API integration\n",
    "- Planning, reasoning, and decision-making algorithms\n",
    "- Building production-ready AI agents\n",
    "\n",
    "### üéØ Success Milestones\n",
    "\n",
    "By the end of each phase, you should be able to:\n",
    "\n",
    "**Weeks 1-2:** ‚úÖ Implement attention mechanisms from mathematical foundations  \n",
    "**Weeks 3-4:** ‚úÖ Build complete transformer encoder and decoder architectures  \n",
    "**Week 5:** ‚úÖ Code a full mini-transformer and understand training dynamics  \n",
    "**Weeks 6-7:** ‚úÖ Understand modern LLM architectures and fine-tuning techniques  \n",
    "**Weeks 8-9:** ‚úÖ Apply advanced training methods and deploy production systems\n",
    "\n",
    "### üìö Weekly Time Commitment\n",
    "\n",
    "- **Daily:** 2-3 hours of focused learning & coding\n",
    "- **Weekly:** 10-15 hours total commitment\n",
    "- **Flexibility:** Weekend time for review, projects, and catch-up\n",
    "- **Total Journey:** ~135 hours of comprehensive hands-on learning\n",
    "\n",
    "### üéì Learning Approach\n",
    "\n",
    "- **70% Hands-on Coding:** Build everything from scratch\n",
    "- **20% Theory & Concepts:** Deep understanding of fundamentals\n",
    "- **10% Projects & Practice:** Real-world applications and portfolio building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3657c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Knowledge Check\n",
    "def knowledge_check():\n",
    "    \"\"\"Simple interactive quiz to check understanding\"\"\"\n",
    "\n",
    "    questions = [\n",
    "        {\n",
    "            \"question\": \"What is the main difference between traditional AI and generative AI?\",\n",
    "            \"options\": [\"A) Speed\", \"B) Classification vs Creation\", \"C) Model size\", \"D) Programming language\"],\n",
    "            \"correct\": \"B\",\n",
    "            \"explanation\": \"Traditional AI classifies/analyzes existing data, while Generative AI creates new data.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What breakthrough paper introduced the Transformer architecture?\",\n",
    "            \"options\": [\"A) BERT\", \"B) GPT\", \"C) Attention Is All You Need\", \"D) ResNet\"],\n",
    "            \"correct\": \"C\",\n",
    "            \"explanation\": \"The 2017 paper 'Attention Is All You Need' by Vaswani et al. introduced Transformers.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What allows Transformers to process sequences in parallel?\",\n",
    "            \"options\": [\"A) RNNs\", \"B) Attention mechanism\", \"C) Convolutions\", \"D) Memory\"],\n",
    "            \"correct\": \"B\",\n",
    "            \"explanation\": \"The attention mechanism allows all positions to be processed simultaneously.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    score = 0\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"\\nüìù Question {i}: {q['question']}\")\n",
    "        for option in q['options']:\n",
    "            print(f\"   {option}\")\n",
    "\n",
    "        # In a real interactive environment, you'd get user input\n",
    "        # For now, just show the answer\n",
    "        print(f\"\\n‚úÖ Correct Answer: {q['correct']}\")\n",
    "        print(f\"üí° Explanation: {q['explanation']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(f\"\\nüéâ Great job working through the concepts!\")\n",
    "    print(\"You're ready to move to Chapter 02: Setting Up Your Development Environment\")\n",
    "\n",
    "\n",
    "# Run the knowledge check\n",
    "knowledge_check()\n",
    "\n",
    "# What's next?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"1. üìÅ Navigate to src/chapter-02/ for environment setup\")\n",
    "print(\"2. üîß Install transformer libraries (torch, transformers)\")\n",
    "print(\"3. üß† Start implementing attention mechanisms\")\n",
    "print(\"4. üìö Review the docs/ folder for deep theoretical background\")\n",
    "print()\n",
    "print(\"üí™ Remember: This is a marathon, not a sprint!\")\n",
    "print(\"   Take your time, experiment, and most importantly - have fun!\")\n",
    "print()\n",
    "print(\"üéØ Ready to become a Generative AI expert? Let's go!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
