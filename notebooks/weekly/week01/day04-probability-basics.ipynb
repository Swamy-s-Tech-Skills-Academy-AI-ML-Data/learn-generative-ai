{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372d62d7",
   "metadata": {},
   "source": [
    "# Day 4: Mathematical Foundations I - Probability Basics\n",
    "\n",
    "**Learning Objective**: Understand how probability drives text generation in language models.\n",
    "\n",
    "**Time**: 15 minutes of hands-on practice\n",
    "\n",
    "**Prerequisites**: Read [Day 4 Guide](../../../docs/daily-guides/week01/day04-probability-basics.md) first (10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e5ff7",
   "metadata": {},
   "source": [
    "## üéØ Today's Focus: Probability in Language Generation\n",
    "\n",
    "Let's explore how probability makes text generation possible through interactive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6dc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries for probability exploration\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "print(\"üöÄ Day 4 Environment Ready!\")\n",
    "print(\"Today we'll explore: Probability in Language Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05416bb",
   "metadata": {},
   "source": [
    "## üé≤ Basic Probability Concepts\n",
    "\n",
    "Let's start with fundamental probability concepts using simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_basics_demo():\n",
    "    \"\"\"\n",
    "    Demonstrates basic probability concepts with simple examples.\n",
    "    \"\"\"\n",
    "    print(\"üé≤ PROBABILITY BASICS\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    # Simple coin flip probability\n",
    "    print(\"\\nü™ô Coin Flip Example:\")\n",
    "    print(\"   Possible outcomes: ['Heads', 'Tails']\")\n",
    "    print(\"   Probability of Heads: 0.5 (50%)\")\n",
    "    print(\"   Probability of Tails: 0.5 (50%)\")\n",
    "    print(\"   Total probability: 0.5 + 0.5 = 1.0 (100%)\")\n",
    "\n",
    "    # Word prediction probability\n",
    "    print(\"\\nüìù Word Prediction Example:\")\n",
    "    next_word_probs = {\n",
    "        'good': 0.4,\n",
    "        'great': 0.3,\n",
    "        'excellent': 0.2,\n",
    "        'bad': 0.1\n",
    "    }\n",
    "\n",
    "    print(\"   Context: 'The weather is ___'\")\n",
    "    print(\"   Possible next words and their probabilities:\")\n",
    "    for word, prob in next_word_probs.items():\n",
    "        print(f\"      '{word}': {prob} ({prob*100}%)\")\n",
    "\n",
    "    total_prob = sum(next_word_probs.values())\n",
    "    print(f\"   Total probability: {total_prob} (must equal 1.0)\")\n",
    "\n",
    "    print(\"\\nüí° Key insight: AI assigns probabilities to every possible next word\")\n",
    "\n",
    "    return next_word_probs\n",
    "\n",
    "\n",
    "word_probabilities = probability_basics_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48984cef",
   "metadata": {},
   "source": [
    "## üìä Visualizing Probability Distributions\n",
    "\n",
    "Let's visualize how probabilities look and how they affect text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e71323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_probability_distribution(probabilities: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    Creates a visual representation of probability distributions.\n",
    "    \"\"\"\n",
    "    print(\"üìä VISUALIZING PROBABILITY DISTRIBUTION\")\n",
    "\n",
    "    words = list(probabilities.keys())\n",
    "    probs = list(probabilities.values())\n",
    "\n",
    "    # Text-based visualization\n",
    "    print(\"\\nüìà Probability Bar Chart (text version):\")\n",
    "    max_prob = max(probs)\n",
    "\n",
    "    for word, prob in probabilities.items():\n",
    "        bar_length = int((prob / max_prob) * 20)  # Scale to 20 characters\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "        print(f\"   {word:10} {bar} {prob:.1f}\")\n",
    "\n",
    "    # Simulation: Generate words based on probabilities\n",
    "    print(\"\\nüé≤ Simulation: 10 random selections based on probabilities:\")\n",
    "    selections = []\n",
    "    for i in range(10):\n",
    "        chosen_word = random.choices(words, weights=probs)[0]\n",
    "        selections.append(chosen_word)\n",
    "\n",
    "    selection_counts = Counter(selections)\n",
    "    print(f\"   Selected words: {selections}\")\n",
    "    print(f\"   Selection counts: {dict(selection_counts)}\")\n",
    "\n",
    "    print(\"\\nüí° Notice: Higher probability words appear more often (but not always!)\")\n",
    "\n",
    "    return selections\n",
    "\n",
    "\n",
    "selections = visualize_probability_distribution(word_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ddee3",
   "metadata": {},
   "source": [
    "## üîó Conditional Probability in Language\n",
    "\n",
    "Language models use conditional probability - the probability of a word given the previous context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probability_demo():\n",
    "    \"\"\"\n",
    "    Demonstrates conditional probability in language context.\n",
    "    \"\"\"\n",
    "    print(\"üîó CONDITIONAL PROBABILITY IN LANGUAGE\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    # Different contexts lead to different probability distributions\n",
    "    contexts = {\n",
    "        \"The weather is\": {\n",
    "            'sunny': 0.3,\n",
    "            'rainy': 0.2,\n",
    "            'cloudy': 0.2,\n",
    "            'good': 0.15,\n",
    "            'terrible': 0.15\n",
    "        },\n",
    "        \"The dog is\": {\n",
    "            'running': 0.25,\n",
    "            'sleeping': 0.25,\n",
    "            'barking': 0.2,\n",
    "            'good': 0.15,\n",
    "            'hungry': 0.15\n",
    "        },\n",
    "        \"I am\": {\n",
    "            'happy': 0.3,\n",
    "            'tired': 0.25,\n",
    "            'learning': 0.2,\n",
    "            'good': 0.15,\n",
    "            'confused': 0.1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"Context affects probability! Same word, different contexts:\")\n",
    "\n",
    "    for context, word_probs in contexts.items():\n",
    "        print(f\"\\nüìù Context: '{context} ___'\")\n",
    "        print(\"   Top 3 likely next words:\")\n",
    "\n",
    "        # Sort by probability and show top 3\n",
    "        sorted_words = sorted(word_probs.items(),\n",
    "                              key=lambda x: x[1], reverse=True)[:3]\n",
    "        for word, prob in sorted_words:\n",
    "            print(f\"      '{word}': {prob} ({prob*100:.0f}%)\")\n",
    "\n",
    "    print(\"\\nüéØ KEY INSIGHT: P(word | context) - Probability depends on what came before!\")\n",
    "    print(\"\\nüí° This is why 'good' has different probabilities in different contexts\")\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "context_probabilities = conditional_probability_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff9e43",
   "metadata": {},
   "source": [
    "## ‚ö° Interactive Probability Exploration\n",
    "\n",
    "Let's create our own probability-based text generator to see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c225b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_probability_generator(context: str, num_words: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Interactive probability-based text generator.\n",
    "    Shows how probability drives text generation step by step.\n",
    "    \"\"\"\n",
    "    print(f\"‚ö° INTERACTIVE PROBABILITY GENERATOR\")\n",
    "    print(f\"Starting context: '{context}'\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Simple probability model based on last word\n",
    "    word_transitions = {\n",
    "        'the': {'cat': 0.3, 'dog': 0.3, 'weather': 0.2, 'AI': 0.2},\n",
    "        'cat': {'is': 0.4, 'runs': 0.3, 'sleeps': 0.3},\n",
    "        'dog': {'is': 0.4, 'runs': 0.3, 'barks': 0.3},\n",
    "        'weather': {'is': 0.6, 'looks': 0.4},\n",
    "        'AI': {'is': 0.5, 'learns': 0.3, 'helps': 0.2},\n",
    "        'is': {'good': 0.25, 'great': 0.25, 'running': 0.2, 'sunny': 0.15, 'learning': 0.15},\n",
    "        'runs': {'fast': 0.4, 'quickly': 0.3, 'away': 0.3},\n",
    "        'sleeps': {'peacefully': 0.5, 'quietly': 0.3, 'soundly': 0.2}\n",
    "    }\n",
    "\n",
    "    current_text = context\n",
    "    words = current_text.lower().split()\n",
    "\n",
    "    for step in range(num_words):\n",
    "        print(f\"\\nüîÑ Step {step + 1}:\")\n",
    "        print(f\"   Current text: '{current_text}'\")\n",
    "\n",
    "        if words:\n",
    "            last_word = words[-1]\n",
    "            print(f\"   Looking at last word: '{last_word}'\")\n",
    "\n",
    "            if last_word in word_transitions:\n",
    "                possible_words = word_transitions[last_word]\n",
    "                print(f\"   Possible next words: {possible_words}\")\n",
    "\n",
    "                # Show probability decision process\n",
    "                print(\"   üé≤ Rolling probability dice...\")\n",
    "\n",
    "                # Generate random number and show selection process\n",
    "                random_value = random.random()\n",
    "                print(f\"   Random value: {random_value:.3f}\")\n",
    "\n",
    "                # Weighted selection\n",
    "                word_list = list(possible_words.keys())\n",
    "                weight_list = list(possible_words.values())\n",
    "                chosen_word = random.choices(word_list, weights=weight_list)[0]\n",
    "\n",
    "                print(\n",
    "                    f\"   ‚úÖ Selected: '{chosen_word}' (probability: {possible_words[chosen_word]})\")\n",
    "\n",
    "                current_text += \" \" + chosen_word\n",
    "                words.append(chosen_word)\n",
    "            else:\n",
    "                # Fallback for unknown words\n",
    "                fallback_words = ['and', 'the', 'is', 'very']\n",
    "                chosen_word = random.choice(fallback_words)\n",
    "                print(f\"   ‚ö†Ô∏è Unknown word, using fallback: '{chosen_word}'\")\n",
    "                current_text += \" \" + chosen_word\n",
    "                words.append(chosen_word)\n",
    "\n",
    "        print(f\"   üìù Updated text: '{current_text}'\")\n",
    "\n",
    "    print(f\"\\nüéâ FINAL GENERATED TEXT: '{current_text}'\")\n",
    "    print(\"\\nüí° This shows how probability guides each word choice!\")\n",
    "\n",
    "    return current_text\n",
    "\n",
    "\n",
    "# Test the interactive generator\n",
    "generated_text = interactive_probability_generator(\"The cat\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504e90c",
   "metadata": {},
   "source": [
    "## üéØ Temperature: Controlling Randomness\n",
    "\n",
    "Let's explore how 'temperature' affects probability distributions and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bcedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_effects_demo():\n",
    "    \"\"\"\n",
    "    Demonstrates how temperature affects probability distributions.\n",
    "    \"\"\"\n",
    "    print(\"üå°Ô∏è TEMPERATURE EFFECTS ON PROBABILITY\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    # Original probabilities\n",
    "    original_probs = {'good': 0.4, 'great': 0.3,\n",
    "                      'excellent': 0.2, 'amazing': 0.1}\n",
    "\n",
    "    def apply_temperature(probs: Dict[str, float], temperature: float) -> Dict[str, float]:\n",
    "        \"\"\"Apply temperature scaling to probabilities.\"\"\"\n",
    "        if temperature == 0:\n",
    "            # Greedy: pick highest probability\n",
    "            max_word = max(probs.items(), key=lambda x: x[1])[0]\n",
    "            return {word: (1.0 if word == max_word else 0.0) for word in probs}\n",
    "\n",
    "        # Apply temperature scaling (simplified)\n",
    "        scaled_probs = {}\n",
    "        for word, prob in probs.items():\n",
    "            scaled_probs[word] = prob ** (1/temperature)\n",
    "\n",
    "        # Normalize to sum to 1\n",
    "        total = sum(scaled_probs.values())\n",
    "        return {word: prob/total for word, prob in scaled_probs.items()}\n",
    "\n",
    "    temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "    print(f\"Original probabilities: {original_probs}\")\n",
    "    print(\"\\nüå°Ô∏è Effect of different temperatures:\")\n",
    "\n",
    "    for temp in temperatures:\n",
    "        if temp == 0:\n",
    "            temp_probs = apply_temperature(\n",
    "                original_probs, 0.01)  # Very low temp\n",
    "        else:\n",
    "            temp_probs = apply_temperature(original_probs, temp)\n",
    "\n",
    "        print(f\"\\n   Temperature {temp}:\")\n",
    "        for word, prob in temp_probs.items():\n",
    "            print(f\"      '{word}': {prob:.3f}\")\n",
    "\n",
    "        # Describe the effect\n",
    "        if temp <= 0.5:\n",
    "            print(\"      ‚Üí More predictable, conservative choices\")\n",
    "        elif temp <= 1.0:\n",
    "            print(\"      ‚Üí Balanced creativity and predictability\")\n",
    "        else:\n",
    "            print(\"      ‚Üí More creative, diverse choices\")\n",
    "\n",
    "    print(\"\\nüí° Key insights:\")\n",
    "    print(\"   ‚Ä¢ Low temperature ‚Üí More predictable text\")\n",
    "    print(\"   ‚Ä¢ High temperature ‚Üí More creative/random text\")\n",
    "    print(\"   ‚Ä¢ Temperature = 1.0 ‚Üí Use original probabilities\")\n",
    "\n",
    "    return temperatures\n",
    "\n",
    "\n",
    "temperature_effects_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec906f",
   "metadata": {},
   "source": [
    "## üèÜ Day 4 Knowledge Check\n",
    "\n",
    "Test your understanding of probability in language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day4_knowledge_check():\n",
    "    \"\"\"\n",
    "    Interactive knowledge check for Day 4 probability concepts.\n",
    "    \"\"\"\n",
    "    print(\"üìã Day 4 Knowledge Check: Probability in Language Models\")\n",
    "\n",
    "    probability_quiz = [\n",
    "        (\"What must all probabilities in a distribution sum to?\",\n",
    "         \"1.0 (100%)\", \"Complete certainty across all possibilities\"),\n",
    "        (\"How does context affect word probabilities?\", \"Changes the distribution\",\n",
    "         \"P(word|context) varies with different contexts\"),\n",
    "        (\"What happens with low temperature (0.1)?\", \"More predictable output\",\n",
    "         \"Concentrates probability on likely words\"),\n",
    "        (\"What happens with high temperature (2.0)?\",\n",
    "         \"More creative output\", \"Spreads probability more evenly\"),\n",
    "        (\"Why don't language models always pick the highest probability word?\",\n",
    "         \"To avoid repetition\", \"Sampling adds variety and creativity\")\n",
    "    ]\n",
    "\n",
    "    print(\"\\nProbability concepts in language generation:\")\n",
    "\n",
    "    for i, (question, answer, explanation) in enumerate(probability_quiz, 1):\n",
    "        print(f\"\\n{i}. Q: {question}\")\n",
    "        print(f\"   A: {answer}\")\n",
    "        print(f\"   Why: {explanation}\")\n",
    "\n",
    "    print(\"\\nüéØ If you understand these probability concepts, you've mastered Day 4!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "day4_knowledge_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363bc786",
   "metadata": {},
   "source": [
    "## üìù Day 4 Reflection (5 minutes)\n",
    "\n",
    "Reflect on probability in language generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af835a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Day 4 Reflection Questions:\")\n",
    "print(\"\\n1. How does probability make language generation possible?\")\n",
    "print(\"   Your answer: [Write your explanation here]\")\n",
    "\n",
    "print(\"\\n2. Why is conditional probability important for coherent text?\")\n",
    "print(\"   Your answer: [Write your reasoning here]\")\n",
    "\n",
    "print(\"\\n3. When would you prefer low vs high temperature settings?\")\n",
    "print(\"   Your answer: [Write your use cases here]\")\n",
    "\n",
    "print(\"\\nüéØ Tomorrow: We'll explore LOSS FUNCTIONS - how AI learns to improve\")\n",
    "print(\"üìñ Next guide: Day 5 - Mathematical Foundations II (Loss Functions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bf9a2",
   "metadata": {},
   "source": [
    "## ‚úÖ Day 4 Completion Checklist\n",
    "\n",
    "Before moving to Day 5, confirm you can:\n",
    "\n",
    "- [ ] Explain how probability drives text generation\n",
    "- [ ] Understand conditional probability P(word|context)\n",
    "- [ ] Describe how temperature affects creativity\n",
    "- [ ] Recognize why probabilities must sum to 1.0\n",
    "- [ ] Connect probability concepts to language model behavior\n",
    "\n",
    "**üéâ Day 4 Complete!** Ready for [Day 5: Loss Functions](day05-loss-functions.ipynb)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdae305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Day 4: Probability in AI - Interactive Exploration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def explore_probability_distributions():\n",
    "    \"\"\"\n",
    "    Interactive exploration of probability distributions used in AI.\n",
    "    \"\"\"\n",
    "    print(\"üé≤ Probability Distributions in AI\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create subplot for different distributions\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Uniform Distribution (baseline for comparison)\n",
    "    x_uniform = np.linspace(0, 1, 1000)\n",
    "    y_uniform = np.ones_like(x_uniform)\n",
    "\n",
    "    ax1.plot(x_uniform, y_uniform, 'b-', linewidth=3,\n",
    "             label='Uniform Distribution')\n",
    "    ax1.fill_between(x_uniform, y_uniform, alpha=0.3, color='blue')\n",
    "    ax1.set_title('Uniform Distribution\\n(Equal probability for all outcomes)')\n",
    "    ax1.set_xlabel('Value')\n",
    "    ax1.set_ylabel('Probability Density')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Normal Distribution (Gaussian)\n",
    "    x_normal = np.linspace(-4, 4, 1000)\n",
    "    y_normal = stats.norm.pdf(x_normal, 0, 1)\n",
    "\n",
    "    ax2.plot(x_normal, y_normal, 'g-', linewidth=3, label='Standard Normal')\n",
    "    ax2.fill_between(x_normal, y_normal, alpha=0.3, color='green')\n",
    "    ax2.axvline(0, color='red', linestyle='--', alpha=0.7, label='Mean')\n",
    "    ax2.set_title('Normal Distribution\\n(Bell curve - common in AI)')\n",
    "    ax2.set_xlabel('Value')\n",
    "    ax2.set_ylabel('Probability Density')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Softmax-like distribution (categorical)\n",
    "    categories = ['Cat', 'Dog', 'Bird', 'Fish', 'Rabbit']\n",
    "    # Simulate model confidence\n",
    "    logits = np.array([2.1, 1.8, 0.5, 0.3, 0.8])\n",
    "    softmax_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "\n",
    "    bars = ax3.bar(categories, softmax_probs, color=[\n",
    "                   'orange', 'brown', 'blue', 'cyan', 'pink'], alpha=0.7)\n",
    "    ax3.set_title('Softmax Distribution\\n(AI model predictions)')\n",
    "    ax3.set_xlabel('Categories')\n",
    "    ax3.set_ylabel('Probability')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Add probability values on bars\n",
    "    for bar, prob in zip(bars, softmax_probs):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Attention weights visualization\n",
    "    sequence_length = 8\n",
    "    attention_weights = np.random.dirichlet(np.ones(sequence_length) * 2)\n",
    "    positions = np.arange(sequence_length)\n",
    "\n",
    "    ax4.plot(positions, attention_weights, 'ro-', linewidth=2,\n",
    "             markersize=8, label='Attention Weights')\n",
    "    ax4.fill_between(positions, attention_weights, alpha=0.3, color='red')\n",
    "    ax4.set_title(\n",
    "        'Attention Probability Distribution\\n(Where the model \"looks\")')\n",
    "    ax4.set_xlabel('Sequence Position')\n",
    "    ax4.set_ylabel('Attention Weight')\n",
    "    ax4.set_xticks(positions)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print probability insights\n",
    "    print(\"\\nüìä Probability Insights:\")\n",
    "    print(\n",
    "        f\"Softmax predictions sum to: {np.sum(softmax_probs):.6f} (should be 1.0)\")\n",
    "    print(\n",
    "        f\"Most confident prediction: {categories[np.argmax(softmax_probs)]} ({softmax_probs.max():.3f})\")\n",
    "    print(\n",
    "        f\"Attention weights sum to: {np.sum(attention_weights):.6f} (should be 1.0)\")\n",
    "    print(\n",
    "        f\"Most attended position: {np.argmax(attention_weights)} ({attention_weights.max():.3f})\")\n",
    "\n",
    "    return {\n",
    "        'softmax_probs': softmax_probs,\n",
    "        'attention_weights': attention_weights,\n",
    "        'categories': categories\n",
    "    }\n",
    "\n",
    "\n",
    "# Run probability exploration\n",
    "prob_data = explore_probability_distributions()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
