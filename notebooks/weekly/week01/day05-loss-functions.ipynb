{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d046d982",
   "metadata": {},
   "source": [
    "# Day 5: Mathematical Foundations II - Loss Functions\n",
    "\n",
    "**Learning Objective**: Understand how AI systems learn to improve their predictions through loss functions.\n",
    "\n",
    "**Time**: 15 minutes of hands-on practice\n",
    "\n",
    "**Prerequisites**: Read [Day 5 Guide](../../../docs/daily-guides/week01/day05-loss-functions.md) first (10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635495f5",
   "metadata": {},
   "source": [
    "## 🎯 Today's Focus: How AI Learns from Mistakes\n",
    "\n",
    "Let's explore how loss functions measure prediction errors and guide learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries for loss function exploration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "print(\"🚀 Day 5 Environment Ready!\")\n",
    "print(\"Today we'll explore: How AI learns through loss functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc6c0c",
   "metadata": {},
   "source": [
    "## 📏 What is a Loss Function?\n",
    "\n",
    "A loss function measures how wrong our AI's predictions are - the bigger the loss, the worse the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_basics():\n",
    "    \"\"\"\n",
    "    Demonstrates basic loss function concepts with simple examples.\n",
    "    \"\"\"\n",
    "    print(\"📏 LOSS FUNCTION BASICS\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    print(\"\\n🎯 Think of loss as 'wrongness score':\")\n",
    "    print(\"   • Perfect prediction → Loss = 0\")\n",
    "    print(\"   • Bad prediction → Loss = HIGH\")\n",
    "    print(\"   • AI goal: Minimize loss (reduce wrongness)\")\n",
    "\n",
    "    # Simple prediction examples\n",
    "    examples = [\n",
    "        (\"Target: 'good', Predicted: 'good'\", 0.0, \"Perfect match!\"),\n",
    "        (\"Target: 'good', Predicted: 'great'\", 0.3, \"Close but not exact\"),\n",
    "        (\"Target: 'good', Predicted: 'terrible'\", 2.5, \"Very wrong prediction\"),\n",
    "        (\"Target: 'happy', Predicted: 'sad'\", 3.0, \"Opposite meaning!\")\n",
    "    ]\n",
    "\n",
    "    print(\"\\n📊 Example prediction losses:\")\n",
    "    for example, loss, explanation in examples:\n",
    "        print(f\"   {example}\")\n",
    "        print(f\"      Loss: {loss} - {explanation}\")\n",
    "\n",
    "    print(\"\\n💡 Key insight: Loss function converts 'wrongness' into numbers AI can optimize\")\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "loss_examples = loss_function_basics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69616c",
   "metadata": {},
   "source": [
    "## 🔥 Cross-Entropy Loss: The Language Model Loss\n",
    "\n",
    "Cross-entropy loss is the most important loss function for language models. Let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb56161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_demo():\n",
    "    \"\"\"\n",
    "    Interactive demonstration of cross-entropy loss in language models.\n",
    "    \"\"\"\n",
    "    print(\"🔥 CROSS-ENTROPY LOSS EXPLAINED\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Scenario: Predicting next word after \"The weather is\"\n",
    "    target_word = \"sunny\"\n",
    "    vocabulary = [\"sunny\", \"rainy\", \"cloudy\", \"cold\"]\n",
    "    target_index = vocabulary.index(target_word)\n",
    "\n",
    "    print(f\"\\n📝 Context: 'The weather is ___'\")\n",
    "    print(f\"🎯 Correct answer: '{target_word}'\")\n",
    "    print(f\"📚 Vocabulary: {vocabulary}\")\n",
    "\n",
    "    # Different prediction scenarios\n",
    "    prediction_scenarios = [\n",
    "        # Very confident in correct answer\n",
    "        (\"Confident Correct\", [0.9, 0.05, 0.03, 0.02]),\n",
    "        (\"Somewhat Correct\", [0.6, 0.2, 0.15, 0.05]),    # Moderately confident\n",
    "        (\"Uncertain\", [0.25, 0.25, 0.25, 0.25]),         # Completely uncertain\n",
    "        # Confident in wrong answer\n",
    "        (\"Wrong but Confident\", [0.1, 0.8, 0.05, 0.05])\n",
    "    ]\n",
    "\n",
    "    def calculate_cross_entropy(predicted_probs: List[float], target_idx: int) -> float:\n",
    "        \"\"\"Calculate cross-entropy loss.\"\"\"\n",
    "        # Cross-entropy: -log(probability of correct answer)\n",
    "        correct_prob = predicted_probs[target_idx]\n",
    "        if correct_prob <= 0:\n",
    "            return float('inf')  # Infinite loss for 0 probability\n",
    "        return -math.log(correct_prob)\n",
    "\n",
    "    print(\"\\n🧮 Cross-entropy loss for different predictions:\")\n",
    "\n",
    "    for scenario_name, predicted_probs in prediction_scenarios:\n",
    "        loss = calculate_cross_entropy(predicted_probs, target_index)\n",
    "        correct_prob = predicted_probs[target_index]\n",
    "\n",
    "        print(f\"\\n   🔸 {scenario_name}:\")\n",
    "        print(f\"      Predicted probabilities: {predicted_probs}\")\n",
    "        print(\n",
    "            f\"      Probability of correct word '{target_word}': {correct_prob}\")\n",
    "        print(f\"      Cross-entropy loss: {loss:.3f}\")\n",
    "\n",
    "        # Interpretation\n",
    "        if loss < 0.5:\n",
    "            print(f\"      📊 Interpretation: EXCELLENT prediction (loss < 0.5)\")\n",
    "        elif loss < 1.0:\n",
    "            print(f\"      📊 Interpretation: Good prediction (loss < 1.0)\")\n",
    "        elif loss < 2.0:\n",
    "            print(f\"      📊 Interpretation: Poor prediction (loss < 2.0)\")\n",
    "        else:\n",
    "            print(f\"      📊 Interpretation: Very bad prediction (loss ≥ 2.0)\")\n",
    "\n",
    "    print(\"\\n💡 Key insights about cross-entropy:\")\n",
    "    print(\"   • Lower loss = better prediction\")\n",
    "    print(\"   • Loss = 0 only when probability = 1.0 (perfect confidence)\")\n",
    "    print(\"   • Loss increases rapidly as confidence in correct answer decreases\")\n",
    "    print(\"   • Punishes overconfident wrong predictions heavily\")\n",
    "\n",
    "    return prediction_scenarios\n",
    "\n",
    "\n",
    "scenarios = cross_entropy_loss_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dec740",
   "metadata": {},
   "source": [
    "## 📈 Visualizing Loss Behavior\n",
    "\n",
    "Let's see how loss changes with prediction confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f095f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_behavior():\n",
    "    \"\"\"\n",
    "    Creates text-based visualization of loss function behavior.\n",
    "    \"\"\"\n",
    "    print(\"📈 LOSS FUNCTION BEHAVIOR VISUALIZATION\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    # Range of confidence levels from very low to perfect\n",
    "    confidence_levels = [0.01, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "    print(\"\\n📊 How cross-entropy loss changes with confidence:\")\n",
    "    print(\"    Confidence | Loss  | Visualization\")\n",
    "    print(\"    ---------- | ----- | -------------\")\n",
    "\n",
    "    for confidence in confidence_levels:\n",
    "        loss = -math.log(confidence)\n",
    "\n",
    "        # Create text-based bar for loss magnitude\n",
    "        bar_length = min(int(loss * 5), 50)  # Scale and cap the bar\n",
    "        bar = \"█\" * bar_length\n",
    "\n",
    "        print(f\"    {confidence:8.2f} | {loss:5.2f} | {bar}\")\n",
    "\n",
    "    print(\"\\n🎯 Pattern observations:\")\n",
    "    print(\"   • Confidence 0.99 → Loss 0.01 (very small loss)\")\n",
    "    print(\"   • Confidence 0.50 → Loss 0.69 (moderate loss)\")\n",
    "    print(\"   • Confidence 0.10 → Loss 2.30 (high loss)\")\n",
    "    print(\"   • Confidence 0.01 → Loss 4.61 (very high loss)\")\n",
    "\n",
    "    print(\"\\n💡 Why this matters for AI training:\")\n",
    "    print(\"   • AI learns by trying to minimize loss\")\n",
    "    print(\"   • This encourages high confidence in correct predictions\")\n",
    "    print(\"   • Penalty grows rapidly for wrong confident predictions\")\n",
    "\n",
    "    return confidence_levels\n",
    "\n",
    "\n",
    "confidence_data = visualize_loss_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe978182",
   "metadata": {},
   "source": [
    "## 🎓 Training Simulation: How AI Learns\n",
    "\n",
    "Let's simulate how an AI model learns to improve its predictions over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c176912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_learning_simulation():\n",
    "    \"\"\"\n",
    "    Simulates how AI learning works through loss minimization.\n",
    "    \"\"\"\n",
    "    print(\"🎓 AI LEARNING SIMULATION\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    # Training scenario: Learning to predict weather words\n",
    "    training_examples = [\n",
    "        (\"The weather is\", \"sunny\"),\n",
    "        (\"It looks\", \"cloudy\"),\n",
    "        (\"Today seems\", \"rainy\")\n",
    "    ]\n",
    "\n",
    "    vocabulary = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n",
    "\n",
    "    print(f\"\\n📚 Training data: {len(training_examples)} examples\")\n",
    "    for context, target in training_examples:\n",
    "        print(f\"   '{context}' → '{target}'\")\n",
    "\n",
    "    # Simulate learning epochs\n",
    "    print(\"\\n🔄 Training epochs (learning iterations):\")\n",
    "\n",
    "    # Start with random predictions (epoch 0)\n",
    "    epochs = [\n",
    "        (\"Epoch 0 (Random)\", [0.25, 0.25, 0.25, 0.25]),  # Random guessing\n",
    "        (\"Epoch 5\", [0.4, 0.3, 0.2, 0.1]),                # Starting to learn\n",
    "        (\"Epoch 10\", [0.6, 0.25, 0.1, 0.05]),             # Getting better\n",
    "        (\"Epoch 20\", [0.8, 0.15, 0.03, 0.02]),            # Much better\n",
    "        (\"Epoch 50\", [0.95, 0.03, 0.01, 0.01])            # Nearly perfect\n",
    "    ]\n",
    "\n",
    "    def calculate_average_loss(probabilities: List[float]) -> float:\n",
    "        \"\"\"Calculate average loss across all training examples.\"\"\"\n",
    "        total_loss = 0\n",
    "        for context, target in training_examples:\n",
    "            target_idx = vocabulary.index(target)\n",
    "            loss = -math.log(probabilities[target_idx])\n",
    "            total_loss += loss\n",
    "        return total_loss / len(training_examples)\n",
    "\n",
    "    for epoch_name, typical_probs in epochs:\n",
    "        avg_loss = calculate_average_loss(typical_probs)\n",
    "\n",
    "        print(f\"\\n   📊 {epoch_name}:\")\n",
    "        print(f\"      Typical predictions: {typical_probs}\")\n",
    "        print(f\"      Average loss: {avg_loss:.3f}\")\n",
    "\n",
    "        # Show improvement\n",
    "        if avg_loss > 2.0:\n",
    "            print(f\"      Status: 😵 Still learning (high loss)\")\n",
    "        elif avg_loss > 1.0:\n",
    "            print(f\"      Status: 🤔 Making progress (medium loss)\")\n",
    "        elif avg_loss > 0.5:\n",
    "            print(f\"      Status: 😊 Getting good (low loss)\")\n",
    "        else:\n",
    "            print(f\"      Status: 🎉 Excellent performance (very low loss)\")\n",
    "\n",
    "    print(\"\\n🎯 Learning process summary:\")\n",
    "    print(\"   1. Start with random predictions (high loss)\")\n",
    "    print(\"   2. Gradually adjust to minimize loss\")\n",
    "    print(\"   3. Eventually learn correct patterns (low loss)\")\n",
    "    print(\"   4. Loss decreases = better predictions = smarter AI\")\n",
    "\n",
    "    return epochs\n",
    "\n",
    "\n",
    "learning_epochs = ai_learning_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6493e0",
   "metadata": {},
   "source": [
    "## 🔍 Different Types of Loss Functions\n",
    "\n",
    "Cross-entropy isn't the only loss function. Let's explore others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_comparison():\n",
    "    \"\"\"\n",
    "    Compares different types of loss functions and their uses.\n",
    "    \"\"\"\n",
    "    print(\"🔍 LOSS FUNCTION COMPARISON\")\n",
    "    print(\"=\"*35)\n",
    "\n",
    "    # Example: Predicting a number (regression) vs predicting a category (classification)\n",
    "\n",
    "    print(\"\\n📊 Different tasks need different loss functions:\")\n",
    "\n",
    "    # Mean Squared Error (MSE) for numbers\n",
    "    print(\"\\n1️⃣ Mean Squared Error (MSE) - For predicting numbers:\")\n",
    "    number_examples = [\n",
    "        (\"Target: 5.0, Predicted: 5.0\", 0.0),\n",
    "        (\"Target: 5.0, Predicted: 4.0\", 1.0),\n",
    "        (\"Target: 5.0, Predicted: 3.0\", 4.0),\n",
    "        (\"Target: 5.0, Predicted: 1.0\", 16.0)\n",
    "    ]\n",
    "\n",
    "    for example, mse_loss in number_examples:\n",
    "        print(f\"   {example} → MSE Loss: {mse_loss}\")\n",
    "\n",
    "    print(\"   💡 MSE = (target - prediction)²\")\n",
    "    print(\"   Use case: Predicting temperatures, prices, ratings\")\n",
    "\n",
    "    # Cross-entropy for categories\n",
    "    print(\"\\n2️⃣ Cross-Entropy Loss - For predicting categories:\")\n",
    "    category_examples = [\n",
    "        (\"Target: 'dog', Confident correct\", 0.1),\n",
    "        (\"Target: 'dog', Somewhat correct\", 0.5),\n",
    "        (\"Target: 'dog', Uncertain\", 1.4),\n",
    "        (\"Target: 'dog', Wrong & confident\", 2.3)\n",
    "    ]\n",
    "\n",
    "    for example, ce_loss in category_examples:\n",
    "        print(f\"   {example} → CE Loss: {ce_loss}\")\n",
    "\n",
    "    print(\"   💡 Cross-entropy = -log(probability of correct category)\")\n",
    "    print(\"   Use case: Word prediction, image classification, sentiment analysis\")\n",
    "\n",
    "    # Binary Cross-Entropy for yes/no\n",
    "    print(\"\\n3️⃣ Binary Cross-Entropy - For yes/no predictions:\")\n",
    "    binary_examples = [\n",
    "        (\"Target: Yes, Predicted: 90% yes\", 0.15),\n",
    "        (\"Target: Yes, Predicted: 70% yes\", 0.51),\n",
    "        (\"Target: Yes, Predicted: 30% yes\", 1.67),\n",
    "        (\"Target: Yes, Predicted: 10% yes\", 3.32)\n",
    "    ]\n",
    "\n",
    "    for example, bce_loss in binary_examples:\n",
    "        print(f\"   {example} → BCE Loss: {bce_loss:.2f}\")\n",
    "\n",
    "    print(\"   💡 Binary CE = -[y×log(p) + (1-y)×log(1-p)]\")\n",
    "    print(\"   Use case: Spam detection, medical diagnosis, sentiment (positive/negative)\")\n",
    "\n",
    "    print(\"\\n🎯 Key principle: Choose loss function based on your task type!\")\n",
    "\n",
    "    return {\"mse\": number_examples, \"ce\": category_examples, \"bce\": binary_examples}\n",
    "\n",
    "\n",
    "loss_types = loss_function_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b90ed2",
   "metadata": {},
   "source": [
    "## 🏆 Day 5 Knowledge Check\n",
    "\n",
    "Test your understanding of loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42aaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day5_knowledge_check():\n",
    "    \"\"\"\n",
    "    Interactive knowledge check for Day 5 loss function concepts.\n",
    "    \"\"\"\n",
    "    print(\"📋 Day 5 Knowledge Check: Loss Functions in AI Learning\")\n",
    "\n",
    "    loss_quiz = [\n",
    "        (\"What does a loss function measure?\", \"How wrong predictions are\",\n",
    "         \"Converts prediction errors into numbers\"),\n",
    "        (\"What happens to loss when predictions improve?\",\n",
    "         \"Loss decreases\", \"Better predictions = lower loss scores\"),\n",
    "        (\"What's the loss when prediction is perfect?\",\n",
    "         \"Loss = 0\", \"Perfect prediction means no error\"),\n",
    "        (\"Which loss function do language models use?\",\n",
    "         \"Cross-entropy loss\", \"Measures probability prediction errors\"),\n",
    "        (\"How does AI learn?\", \"By minimizing loss\",\n",
    "         \"Adjusts to reduce prediction errors over time\"),\n",
    "        (\"What's the goal of training?\", \"Minimize average loss\",\n",
    "         \"Find patterns that reduce errors consistently\")\n",
    "    ]\n",
    "\n",
    "    print(\"\\nLoss function concepts in AI learning:\")\n",
    "\n",
    "    for i, (question, answer, explanation) in enumerate(loss_quiz, 1):\n",
    "        print(f\"\\n{i}. Q: {question}\")\n",
    "        print(f\"   A: {answer}\")\n",
    "        print(f\"   Why: {explanation}\")\n",
    "\n",
    "    print(\"\\n🧮 Quick calculation check:\")\n",
    "    print(\"   If AI predicts 'sunny' with 80% confidence and it's correct:\")\n",
    "    print(f\"   Cross-entropy loss = -log(0.8) = {-math.log(0.8):.3f}\")\n",
    "    print(\"   (Lower confidence → Higher loss)\")\n",
    "\n",
    "    print(\"\\n🎯 If you understand these loss concepts, you've mastered Day 5!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "day5_knowledge_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11aa4ce",
   "metadata": {},
   "source": [
    "## 📝 Day 5 Reflection (5 minutes)\n",
    "\n",
    "Reflect on how AI learns through loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📝 Day 5 Reflection Questions:\")\n",
    "print(\"\\n1. How do loss functions enable AI to learn?\")\n",
    "print(\"   Your answer: [Write your explanation here]\")\n",
    "\n",
    "print(\"\\n2. Why is cross-entropy loss particularly good for language models?\")\n",
    "print(\"   Your answer: [Write your reasoning here]\")\n",
    "\n",
    "print(\"\\n3. What would happen if we used the wrong loss function for a task?\")\n",
    "print(\"   Your answer: [Write your prediction here]\")\n",
    "\n",
    "print(\"\\n🎯 Week 1 Complete! You've built a solid foundation in GenAI fundamentals\")\n",
    "print(\"📖 Next week: We'll dive deeper into tokenization and text processing\")\n",
    "print(\"🎉 Great progress - you understand the mathematical foundations of AI learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c004e72",
   "metadata": {},
   "source": [
    "## ✅ Day 5 & Week 1 Completion Checklist\n",
    "\n",
    "Before moving to Week 2, confirm you can:\n",
    "\n",
    "### Day 5 Concepts:\n",
    "- [ ] Explain what loss functions measure\n",
    "- [ ] Understand cross-entropy loss for language models\n",
    "- [ ] Describe how AI learns by minimizing loss\n",
    "- [ ] Choose appropriate loss functions for different tasks\n",
    "- [ ] Connect loss minimization to model improvement\n",
    "\n",
    "### Week 1 Foundation:\n",
    "- [ ] Distinguish generative vs discriminative AI\n",
    "- [ ] Understand training vs generation phases\n",
    "- [ ] Identify key GenAI components (tokenizer, neural network, sampling)\n",
    "- [ ] Explain probability's role in text generation\n",
    "- [ ] Grasp how loss functions drive learning\n",
    "\n",
    "**🎉 Week 1 Complete!** Ready for [Week 2: Tokenization Deep Dive](../../week02/)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8101d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Day 5: Loss Functions - The AI Learning Signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def explore_loss_functions():\n",
    "    \"\"\"\n",
    "    Interactive exploration of different loss functions and their behaviors.\n",
    "    \"\"\"\n",
    "    print(\"📈 Loss Functions Explorer\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Generate predictions and targets for demonstration\n",
    "    predictions = np.linspace(0, 1, 100)\n",
    "    target = 0.7  # True value\n",
    "\n",
    "    # Calculate different loss functions\n",
    "    mse_loss = (predictions - target) ** 2\n",
    "    mae_loss = np.abs(predictions - target)\n",
    "\n",
    "    # Cross-entropy for binary classification\n",
    "    epsilon = 1e-15  # Prevent log(0)\n",
    "    predictions_clipped = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    binary_ce_loss = -(target * np.log(predictions_clipped) +\n",
    "                       (1 - target) * np.log(1 - predictions_clipped))\n",
    "\n",
    "    # Huber loss (smooth combination of MSE and MAE)\n",
    "    delta = 0.1\n",
    "    huber_loss = np.where(np.abs(predictions - target) <= delta,\n",
    "                          0.5 * (predictions - target) ** 2,\n",
    "                          delta * (np.abs(predictions - target) - 0.5 * delta))\n",
    "\n",
    "    # Create comprehensive visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot loss functions\n",
    "    ax1.plot(predictions, mse_loss, 'b-', linewidth=3, label='MSE Loss')\n",
    "    ax1.plot(predictions, mae_loss, 'r-', linewidth=3, label='MAE Loss')\n",
    "    ax1.plot(predictions, huber_loss, 'g-', linewidth=3, label='Huber Loss')\n",
    "    ax1.axvline(target, color='black', linestyle='--',\n",
    "                alpha=0.7, label=f'Target = {target}')\n",
    "    ax1.set_xlabel('Prediction')\n",
    "    ax1.set_ylabel('Loss Value')\n",
    "    ax1.set_title('Regression Loss Functions Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Cross-entropy visualization\n",
    "    ax2.plot(predictions, binary_ce_loss, 'purple',\n",
    "             linewidth=3, label='Binary Cross-Entropy')\n",
    "    ax2.axvline(target, color='black', linestyle='--',\n",
    "                alpha=0.7, label=f'Target = {target}')\n",
    "    ax2.set_xlabel('Prediction')\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_title('Classification Loss Function')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 5)  # Limit y-axis for better visualization\n",
    "\n",
    "    # Loss gradients (how loss changes with prediction)\n",
    "    mse_gradient = 2 * (predictions - target)\n",
    "    mae_gradient = np.sign(predictions - target)\n",
    "\n",
    "    ax3.plot(predictions, mse_gradient, 'b-',\n",
    "             linewidth=3, label='MSE Gradient')\n",
    "    ax3.plot(predictions, mae_gradient, 'r-',\n",
    "             linewidth=3, label='MAE Gradient')\n",
    "    ax3.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax3.axvline(target, color='black', linestyle='--',\n",
    "                alpha=0.7, label=f'Target = {target}')\n",
    "    ax3.set_xlabel('Prediction')\n",
    "    ax3.set_ylabel('Gradient')\n",
    "    ax3.set_title('Loss Gradients (Learning Signals)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Training simulation\n",
    "    epochs = np.arange(1, 51)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Simulate different learning scenarios\n",
    "    fast_learning = 2.0 * np.exp(-epochs/10) + \\\n",
    "        0.1 + np.random.normal(0, 0.05, len(epochs))\n",
    "    slow_learning = 2.0 * np.exp(-epochs/25) + \\\n",
    "        0.1 + np.random.normal(0, 0.05, len(epochs))\n",
    "    unstable_learning = 2.0 * \\\n",
    "        np.exp(-epochs/15) + 0.1 + np.random.normal(0, 0.2, len(epochs))\n",
    "\n",
    "    ax4.plot(epochs, fast_learning, 'g-', linewidth=2,\n",
    "             label='Good Learning Rate', alpha=0.8)\n",
    "    ax4.plot(epochs, slow_learning, 'b-', linewidth=2,\n",
    "             label='Too Slow Learning Rate', alpha=0.8)\n",
    "    ax4.plot(epochs, unstable_learning, 'r-', linewidth=2,\n",
    "             label='Too Fast Learning Rate', alpha=0.8)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Loss Value')\n",
    "    ax4.set_title('Learning Rate Effects on Training')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'losses': {\n",
    "            'mse': mse_loss,\n",
    "            'mae': mae_loss,\n",
    "            'huber': huber_loss,\n",
    "            'cross_entropy': binary_ce_loss\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Run loss function exploration\n",
    "loss_data = explore_loss_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5574c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_ai_limitations_and_ethics():\n",
    "    \"\"\"\n",
    "    Educational exploration of AI limitations and ethical considerations.\n",
    "    Critical awareness for responsible AI use.\n",
    "    \"\"\"\n",
    "    print(\"⚠️  AI Limitations and Ethics Awareness\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create visualization of AI limitations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # 1. Hallucination frequency simulation\n",
    "    scenarios = ['Facts', 'Dates', 'Sources', 'Statistics', 'Quotes']\n",
    "    hallucination_rates = [0.15, 0.25, 0.35, 0.20, 0.30]  # Simulated rates\n",
    "\n",
    "    bars1 = ax1.bar(scenarios, hallucination_rates, color=[\n",
    "                    'red', 'orange', 'darkred', 'coral', 'crimson'], alpha=0.7)\n",
    "    ax1.set_title('AI Hallucination Risk by Content Type',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Estimated Risk Level')\n",
    "    ax1.set_ylim(0, 0.4)\n",
    "\n",
    "    # Add risk level labels\n",
    "    for bar, rate in zip(bars1, hallucination_rates):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{rate:.0%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Context window limitations\n",
    "    context_lengths = ['GPT-3.5', 'GPT-4', 'Claude-1', 'Claude-2', 'Llama-2']\n",
    "    token_limits = [4096, 8192, 9000, 100000, 4096]  # Approximate token limits\n",
    "\n",
    "    bars2 = ax2.bar(context_lengths, token_limits, color=[\n",
    "                    'blue', 'darkblue', 'purple', 'darkpurple', 'navy'], alpha=0.7)\n",
    "    ax2.set_title('Context Window Limits by Model',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Token Limit')\n",
    "    ax2.set_yscale('log')\n",
    "\n",
    "    # Add token limit labels\n",
    "    for bar, limit in zip(bars2, token_limits):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "                 f'{limit:,}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Bias detection simulation\n",
    "    bias_categories = ['Gender', 'Race', 'Age', 'Religion', 'Nationality']\n",
    "    # Simulated bias detection scores\n",
    "    bias_scores = [0.65, 0.78, 0.45, 0.55, 0.70]\n",
    "\n",
    "    colors = ['red' if score > 0.6 else 'orange' if score >\n",
    "              0.4 else 'green' for score in bias_scores]\n",
    "    bars3 = ax3.bar(bias_categories, bias_scores, color=colors, alpha=0.7)\n",
    "    ax3.set_title('Bias Detection Scores (Higher = More Bias)',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Bias Score (0-1)')\n",
    "    ax3.axhline(y=0.6, color='red', linestyle='--',\n",
    "                alpha=0.7, label='High Risk Threshold')\n",
    "    ax3.axhline(y=0.4, color='orange', linestyle='--',\n",
    "                alpha=0.7, label='Medium Risk Threshold')\n",
    "\n",
    "    # Add bias score labels\n",
    "    for bar, score in zip(bars3, bias_scores):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                 f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    ax3.legend()\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Ethical framework priorities\n",
    "    ethics_principles = ['Accuracy', 'Fairness',\n",
    "                         'Privacy', 'Transparency', 'Accountability']\n",
    "    importance_scores = [9.2, 8.8, 8.5, 7.8, 8.9]  # Importance ratings\n",
    "\n",
    "    bars4 = ax4.barh(ethics_principles, importance_scores, color=[\n",
    "                     'green', 'blue', 'purple', 'orange', 'red'], alpha=0.7)\n",
    "    ax4.set_title('AI Ethics Framework Priorities',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlabel('Importance Score (1-10)')\n",
    "    ax4.set_xlim(0, 10)\n",
    "\n",
    "    # Add importance score labels\n",
    "    for bar, score in zip(bars4, importance_scores):\n",
    "        width = bar.get_width()\n",
    "        ax4.text(width + 0.1, bar.get_y() + bar.get_height()/2.,\n",
    "                 f'{score:.1f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print practical guidance\n",
    "    print(\"\\n📋 Practical AI Limitations Checklist:\")\n",
    "    print(\"✅ Always verify facts, dates, and statistics\")\n",
    "    print(\"✅ Be skeptical of specific quotes or citations\")\n",
    "    print(\"✅ Cross-check important information with reliable sources\")\n",
    "    print(\"✅ Monitor outputs for potential bias\")\n",
    "    print(\"✅ Acknowledge AI limitations to users/stakeholders\")\n",
    "\n",
    "    print(\"\\n🛡️ Ethical AI Usage Guidelines:\")\n",
    "    print(\"1. **Transparency**: Disclose when AI is involved\")\n",
    "    print(\"2. **Human Oversight**: Keep humans in the decision loop\")\n",
    "    print(\"3. **Bias Monitoring**: Regularly check for unfair outputs\")\n",
    "    print(\"4. **Privacy Protection**: Safeguard personal information\")\n",
    "    print(\"5. **Accuracy Verification**: Fact-check critical information\")\n",
    "\n",
    "    print(\"\\n⚖️ When NOT to Use AI:\")\n",
    "    print(\"❌ Life-critical medical decisions without human review\")\n",
    "    print(\"❌ Legal advice without lawyer verification\")\n",
    "    print(\"❌ Financial decisions without expert consultation\")\n",
    "    print(\"❌ Academic work without proper attribution\")\n",
    "    print(\"❌ Sensitive personal information processing\")\n",
    "\n",
    "    return {\n",
    "        'hallucination_risks': dict(zip(scenarios, hallucination_rates)),\n",
    "        'context_limits': dict(zip(context_lengths, token_limits)),\n",
    "        'bias_scores': dict(zip(bias_categories, bias_scores)),\n",
    "        'ethics_priorities': dict(zip(ethics_principles, importance_scores))\n",
    "    }\n",
    "\n",
    "\n",
    "# Explore AI limitations and ethics\n",
    "ethics_data = explore_ai_limitations_and_ethics()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
