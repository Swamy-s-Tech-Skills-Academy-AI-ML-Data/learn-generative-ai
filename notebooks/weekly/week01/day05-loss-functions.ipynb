{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d046d982",
   "metadata": {},
   "source": [
    "# Day 5: Mathematical Foundations II - Loss Functions\n",
    "\n",
    "**Learning Objective**: Understand how AI systems learn to improve their predictions through loss functions.\n",
    "\n",
    "**Time**: 15 minutes of hands-on practice\n",
    "\n",
    "**Prerequisites**: Read [Day 5 Guide](../../../docs/daily-guides/week01/day05-loss-functions.md) first (10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635495f5",
   "metadata": {},
   "source": [
    "## üéØ Today's Focus: How AI Learns from Mistakes\n",
    "\n",
    "Let's explore how loss functions measure prediction errors and guide learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries for loss function exploration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "print(\"üöÄ Day 5 Environment Ready!\")\n",
    "print(\"Today we'll explore: How AI learns through loss functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc6c0c",
   "metadata": {},
   "source": [
    "## üìè What is a Loss Function?\n",
    "\n",
    "A loss function measures how wrong our AI's predictions are - the bigger the loss, the worse the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_basics():\n",
    "    \"\"\"\n",
    "    Demonstrates basic loss function concepts with simple examples.\n",
    "    \"\"\"\n",
    "    print(\"üìè LOSS FUNCTION BASICS\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    print(\"\\nüéØ Think of loss as 'wrongness score':\")\n",
    "    print(\"   ‚Ä¢ Perfect prediction ‚Üí Loss = 0\")\n",
    "    print(\"   ‚Ä¢ Bad prediction ‚Üí Loss = HIGH\")\n",
    "    print(\"   ‚Ä¢ AI goal: Minimize loss (reduce wrongness)\")\n",
    "\n",
    "    # Simple prediction examples\n",
    "    examples = [\n",
    "        (\"Target: 'good', Predicted: 'good'\", 0.0, \"Perfect match!\"),\n",
    "        (\"Target: 'good', Predicted: 'great'\", 0.3, \"Close but not exact\"),\n",
    "        (\"Target: 'good', Predicted: 'terrible'\", 2.5, \"Very wrong prediction\"),\n",
    "        (\"Target: 'happy', Predicted: 'sad'\", 3.0, \"Opposite meaning!\")\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüìä Example prediction losses:\")\n",
    "    for example, loss, explanation in examples:\n",
    "        print(f\"   {example}\")\n",
    "        print(f\"      Loss: {loss} - {explanation}\")\n",
    "\n",
    "    print(\"\\nüí° Key insight: Loss function converts 'wrongness' into numbers AI can optimize\")\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "loss_examples = loss_function_basics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69616c",
   "metadata": {},
   "source": [
    "## üî• Cross-Entropy Loss: The Language Model Loss\n",
    "\n",
    "Cross-entropy loss is the most important loss function for language models. Let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb56161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_demo():\n",
    "    \"\"\"\n",
    "    Interactive demonstration of cross-entropy loss in language models.\n",
    "    \"\"\"\n",
    "    print(\"üî• CROSS-ENTROPY LOSS EXPLAINED\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Scenario: Predicting next word after \"The weather is\"\n",
    "    target_word = \"sunny\"\n",
    "    vocabulary = [\"sunny\", \"rainy\", \"cloudy\", \"cold\"]\n",
    "    target_index = vocabulary.index(target_word)\n",
    "\n",
    "    print(f\"\\nüìù Context: 'The weather is ___'\")\n",
    "    print(f\"üéØ Correct answer: '{target_word}'\")\n",
    "    print(f\"üìö Vocabulary: {vocabulary}\")\n",
    "\n",
    "    # Different prediction scenarios\n",
    "    prediction_scenarios = [\n",
    "        # Very confident in correct answer\n",
    "        (\"Confident Correct\", [0.9, 0.05, 0.03, 0.02]),\n",
    "        (\"Somewhat Correct\", [0.6, 0.2, 0.15, 0.05]),    # Moderately confident\n",
    "        (\"Uncertain\", [0.25, 0.25, 0.25, 0.25]),         # Completely uncertain\n",
    "        # Confident in wrong answer\n",
    "        (\"Wrong but Confident\", [0.1, 0.8, 0.05, 0.05])\n",
    "    ]\n",
    "\n",
    "    def calculate_cross_entropy(predicted_probs: List[float], target_idx: int) -> float:\n",
    "        \"\"\"Calculate cross-entropy loss.\"\"\"\n",
    "        # Cross-entropy: -log(probability of correct answer)\n",
    "        correct_prob = predicted_probs[target_idx]\n",
    "        if correct_prob <= 0:\n",
    "            return float('inf')  # Infinite loss for 0 probability\n",
    "        return -math.log(correct_prob)\n",
    "\n",
    "    print(\"\\nüßÆ Cross-entropy loss for different predictions:\")\n",
    "\n",
    "    for scenario_name, predicted_probs in prediction_scenarios:\n",
    "        loss = calculate_cross_entropy(predicted_probs, target_index)\n",
    "        correct_prob = predicted_probs[target_index]\n",
    "\n",
    "        print(f\"\\n   üî∏ {scenario_name}:\")\n",
    "        print(f\"      Predicted probabilities: {predicted_probs}\")\n",
    "        print(\n",
    "            f\"      Probability of correct word '{target_word}': {correct_prob}\")\n",
    "        print(f\"      Cross-entropy loss: {loss:.3f}\")\n",
    "\n",
    "        # Interpretation\n",
    "        if loss < 0.5:\n",
    "            print(f\"      üìä Interpretation: EXCELLENT prediction (loss < 0.5)\")\n",
    "        elif loss < 1.0:\n",
    "            print(f\"      üìä Interpretation: Good prediction (loss < 1.0)\")\n",
    "        elif loss < 2.0:\n",
    "            print(f\"      üìä Interpretation: Poor prediction (loss < 2.0)\")\n",
    "        else:\n",
    "            print(f\"      üìä Interpretation: Very bad prediction (loss ‚â• 2.0)\")\n",
    "\n",
    "    print(\"\\nüí° Key insights about cross-entropy:\")\n",
    "    print(\"   ‚Ä¢ Lower loss = better prediction\")\n",
    "    print(\"   ‚Ä¢ Loss = 0 only when probability = 1.0 (perfect confidence)\")\n",
    "    print(\"   ‚Ä¢ Loss increases rapidly as confidence in correct answer decreases\")\n",
    "    print(\"   ‚Ä¢ Punishes overconfident wrong predictions heavily\")\n",
    "\n",
    "    return prediction_scenarios\n",
    "\n",
    "\n",
    "scenarios = cross_entropy_loss_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dec740",
   "metadata": {},
   "source": [
    "## üìà Visualizing Loss Behavior\n",
    "\n",
    "Let's see how loss changes with prediction confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f095f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_behavior():\n",
    "    \"\"\"\n",
    "    Creates text-based visualization of loss function behavior.\n",
    "    \"\"\"\n",
    "    print(\"üìà LOSS FUNCTION BEHAVIOR VISUALIZATION\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    # Range of confidence levels from very low to perfect\n",
    "    confidence_levels = [0.01, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "    print(\"\\nüìä How cross-entropy loss changes with confidence:\")\n",
    "    print(\"    Confidence | Loss  | Visualization\")\n",
    "    print(\"    ---------- | ----- | -------------\")\n",
    "\n",
    "    for confidence in confidence_levels:\n",
    "        loss = -math.log(confidence)\n",
    "\n",
    "        # Create text-based bar for loss magnitude\n",
    "        bar_length = min(int(loss * 5), 50)  # Scale and cap the bar\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "\n",
    "        print(f\"    {confidence:8.2f} | {loss:5.2f} | {bar}\")\n",
    "\n",
    "    print(\"\\nüéØ Pattern observations:\")\n",
    "    print(\"   ‚Ä¢ Confidence 0.99 ‚Üí Loss 0.01 (very small loss)\")\n",
    "    print(\"   ‚Ä¢ Confidence 0.50 ‚Üí Loss 0.69 (moderate loss)\")\n",
    "    print(\"   ‚Ä¢ Confidence 0.10 ‚Üí Loss 2.30 (high loss)\")\n",
    "    print(\"   ‚Ä¢ Confidence 0.01 ‚Üí Loss 4.61 (very high loss)\")\n",
    "\n",
    "    print(\"\\nüí° Why this matters for AI training:\")\n",
    "    print(\"   ‚Ä¢ AI learns by trying to minimize loss\")\n",
    "    print(\"   ‚Ä¢ This encourages high confidence in correct predictions\")\n",
    "    print(\"   ‚Ä¢ Penalty grows rapidly for wrong confident predictions\")\n",
    "\n",
    "    return confidence_levels\n",
    "\n",
    "\n",
    "confidence_data = visualize_loss_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe978182",
   "metadata": {},
   "source": [
    "## üéì Training Simulation: How AI Learns\n",
    "\n",
    "Let's simulate how an AI model learns to improve its predictions over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c176912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_learning_simulation():\n",
    "    \"\"\"\n",
    "    Simulates how AI learning works through loss minimization.\n",
    "    \"\"\"\n",
    "    print(\"üéì AI LEARNING SIMULATION\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    # Training scenario: Learning to predict weather words\n",
    "    training_examples = [\n",
    "        (\"The weather is\", \"sunny\"),\n",
    "        (\"It looks\", \"cloudy\"),\n",
    "        (\"Today seems\", \"rainy\")\n",
    "    ]\n",
    "\n",
    "    vocabulary = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n",
    "\n",
    "    print(f\"\\nüìö Training data: {len(training_examples)} examples\")\n",
    "    for context, target in training_examples:\n",
    "        print(f\"   '{context}' ‚Üí '{target}'\")\n",
    "\n",
    "    # Simulate learning epochs\n",
    "    print(\"\\nüîÑ Training epochs (learning iterations):\")\n",
    "\n",
    "    # Start with random predictions (epoch 0)\n",
    "    epochs = [\n",
    "        (\"Epoch 0 (Random)\", [0.25, 0.25, 0.25, 0.25]),  # Random guessing\n",
    "        (\"Epoch 5\", [0.4, 0.3, 0.2, 0.1]),                # Starting to learn\n",
    "        (\"Epoch 10\", [0.6, 0.25, 0.1, 0.05]),             # Getting better\n",
    "        (\"Epoch 20\", [0.8, 0.15, 0.03, 0.02]),            # Much better\n",
    "        (\"Epoch 50\", [0.95, 0.03, 0.01, 0.01])            # Nearly perfect\n",
    "    ]\n",
    "\n",
    "    def calculate_average_loss(probabilities: List[float]) -> float:\n",
    "        \"\"\"Calculate average loss across all training examples.\"\"\"\n",
    "        total_loss = 0\n",
    "        for context, target in training_examples:\n",
    "            target_idx = vocabulary.index(target)\n",
    "            loss = -math.log(probabilities[target_idx])\n",
    "            total_loss += loss\n",
    "        return total_loss / len(training_examples)\n",
    "\n",
    "    for epoch_name, typical_probs in epochs:\n",
    "        avg_loss = calculate_average_loss(typical_probs)\n",
    "\n",
    "        print(f\"\\n   üìä {epoch_name}:\")\n",
    "        print(f\"      Typical predictions: {typical_probs}\")\n",
    "        print(f\"      Average loss: {avg_loss:.3f}\")\n",
    "\n",
    "        # Show improvement\n",
    "        if avg_loss > 2.0:\n",
    "            print(f\"      Status: üòµ Still learning (high loss)\")\n",
    "        elif avg_loss > 1.0:\n",
    "            print(f\"      Status: ü§î Making progress (medium loss)\")\n",
    "        elif avg_loss > 0.5:\n",
    "            print(f\"      Status: üòä Getting good (low loss)\")\n",
    "        else:\n",
    "            print(f\"      Status: üéâ Excellent performance (very low loss)\")\n",
    "\n",
    "    print(\"\\nüéØ Learning process summary:\")\n",
    "    print(\"   1. Start with random predictions (high loss)\")\n",
    "    print(\"   2. Gradually adjust to minimize loss\")\n",
    "    print(\"   3. Eventually learn correct patterns (low loss)\")\n",
    "    print(\"   4. Loss decreases = better predictions = smarter AI\")\n",
    "\n",
    "    return epochs\n",
    "\n",
    "\n",
    "learning_epochs = ai_learning_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6493e0",
   "metadata": {},
   "source": [
    "## üîç Different Types of Loss Functions\n",
    "\n",
    "Cross-entropy isn't the only loss function. Let's explore others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_comparison():\n",
    "    \"\"\"\n",
    "    Compares different types of loss functions and their uses.\n",
    "    \"\"\"\n",
    "    print(\"üîç LOSS FUNCTION COMPARISON\")\n",
    "    print(\"=\"*35)\n",
    "\n",
    "    # Example: Predicting a number (regression) vs predicting a category (classification)\n",
    "\n",
    "    print(\"\\nüìä Different tasks need different loss functions:\")\n",
    "\n",
    "    # Mean Squared Error (MSE) for numbers\n",
    "    print(\"\\n1Ô∏è‚É£ Mean Squared Error (MSE) - For predicting numbers:\")\n",
    "    number_examples = [\n",
    "        (\"Target: 5.0, Predicted: 5.0\", 0.0),\n",
    "        (\"Target: 5.0, Predicted: 4.0\", 1.0),\n",
    "        (\"Target: 5.0, Predicted: 3.0\", 4.0),\n",
    "        (\"Target: 5.0, Predicted: 1.0\", 16.0)\n",
    "    ]\n",
    "\n",
    "    for example, mse_loss in number_examples:\n",
    "        print(f\"   {example} ‚Üí MSE Loss: {mse_loss}\")\n",
    "\n",
    "    print(\"   üí° MSE = (target - prediction)¬≤\")\n",
    "    print(\"   Use case: Predicting temperatures, prices, ratings\")\n",
    "\n",
    "    # Cross-entropy for categories\n",
    "    print(\"\\n2Ô∏è‚É£ Cross-Entropy Loss - For predicting categories:\")\n",
    "    category_examples = [\n",
    "        (\"Target: 'dog', Confident correct\", 0.1),\n",
    "        (\"Target: 'dog', Somewhat correct\", 0.5),\n",
    "        (\"Target: 'dog', Uncertain\", 1.4),\n",
    "        (\"Target: 'dog', Wrong & confident\", 2.3)\n",
    "    ]\n",
    "\n",
    "    for example, ce_loss in category_examples:\n",
    "        print(f\"   {example} ‚Üí CE Loss: {ce_loss}\")\n",
    "\n",
    "    print(\"   üí° Cross-entropy = -log(probability of correct category)\")\n",
    "    print(\"   Use case: Word prediction, image classification, sentiment analysis\")\n",
    "\n",
    "    # Binary Cross-Entropy for yes/no\n",
    "    print(\"\\n3Ô∏è‚É£ Binary Cross-Entropy - For yes/no predictions:\")\n",
    "    binary_examples = [\n",
    "        (\"Target: Yes, Predicted: 90% yes\", 0.15),\n",
    "        (\"Target: Yes, Predicted: 70% yes\", 0.51),\n",
    "        (\"Target: Yes, Predicted: 30% yes\", 1.67),\n",
    "        (\"Target: Yes, Predicted: 10% yes\", 3.32)\n",
    "    ]\n",
    "\n",
    "    for example, bce_loss in binary_examples:\n",
    "        print(f\"   {example} ‚Üí BCE Loss: {bce_loss:.2f}\")\n",
    "\n",
    "    print(\"   üí° Binary CE = -[y√ólog(p) + (1-y)√ólog(1-p)]\")\n",
    "    print(\"   Use case: Spam detection, medical diagnosis, sentiment (positive/negative)\")\n",
    "\n",
    "    print(\"\\nüéØ Key principle: Choose loss function based on your task type!\")\n",
    "\n",
    "    return {\"mse\": number_examples, \"ce\": category_examples, \"bce\": binary_examples}\n",
    "\n",
    "\n",
    "loss_types = loss_function_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b90ed2",
   "metadata": {},
   "source": [
    "## üèÜ Day 5 Knowledge Check\n",
    "\n",
    "Test your understanding of loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42aaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day5_knowledge_check():\n",
    "    \"\"\"\n",
    "    Interactive knowledge check for Day 5 loss function concepts.\n",
    "    \"\"\"\n",
    "    print(\"üìã Day 5 Knowledge Check: Loss Functions in AI Learning\")\n",
    "\n",
    "    loss_quiz = [\n",
    "        (\"What does a loss function measure?\", \"How wrong predictions are\",\n",
    "         \"Converts prediction errors into numbers\"),\n",
    "        (\"What happens to loss when predictions improve?\",\n",
    "         \"Loss decreases\", \"Better predictions = lower loss scores\"),\n",
    "        (\"What's the loss when prediction is perfect?\",\n",
    "         \"Loss = 0\", \"Perfect prediction means no error\"),\n",
    "        (\"Which loss function do language models use?\",\n",
    "         \"Cross-entropy loss\", \"Measures probability prediction errors\"),\n",
    "        (\"How does AI learn?\", \"By minimizing loss\",\n",
    "         \"Adjusts to reduce prediction errors over time\"),\n",
    "        (\"What's the goal of training?\", \"Minimize average loss\",\n",
    "         \"Find patterns that reduce errors consistently\")\n",
    "    ]\n",
    "\n",
    "    print(\"\\nLoss function concepts in AI learning:\")\n",
    "\n",
    "    for i, (question, answer, explanation) in enumerate(loss_quiz, 1):\n",
    "        print(f\"\\n{i}. Q: {question}\")\n",
    "        print(f\"   A: {answer}\")\n",
    "        print(f\"   Why: {explanation}\")\n",
    "\n",
    "    print(\"\\nüßÆ Quick calculation check:\")\n",
    "    print(\"   If AI predicts 'sunny' with 80% confidence and it's correct:\")\n",
    "    print(f\"   Cross-entropy loss = -log(0.8) = {-math.log(0.8):.3f}\")\n",
    "    print(\"   (Lower confidence ‚Üí Higher loss)\")\n",
    "\n",
    "    print(\"\\nüéØ If you understand these loss concepts, you've mastered Day 5!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "day5_knowledge_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11aa4ce",
   "metadata": {},
   "source": [
    "## üìù Day 5 Reflection (5 minutes)\n",
    "\n",
    "Reflect on how AI learns through loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Day 5 Reflection Questions:\")\n",
    "print(\"\\n1. How do loss functions enable AI to learn?\")\n",
    "print(\"   Your answer: [Write your explanation here]\")\n",
    "\n",
    "print(\"\\n2. Why is cross-entropy loss particularly good for language models?\")\n",
    "print(\"   Your answer: [Write your reasoning here]\")\n",
    "\n",
    "print(\"\\n3. What would happen if we used the wrong loss function for a task?\")\n",
    "print(\"   Your answer: [Write your prediction here]\")\n",
    "\n",
    "print(\"\\nüéØ Week 1 Complete! You've built a solid foundation in GenAI fundamentals\")\n",
    "print(\"üìñ Next week: We'll dive deeper into tokenization and text processing\")\n",
    "print(\"üéâ Great progress - you understand the mathematical foundations of AI learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c004e72",
   "metadata": {},
   "source": [
    "## ‚úÖ Day 5 & Week 1 Completion Checklist\n",
    "\n",
    "Before moving to Week 2, confirm you can:\n",
    "\n",
    "### Day 5 Concepts:\n",
    "- [ ] Explain what loss functions measure\n",
    "- [ ] Understand cross-entropy loss for language models\n",
    "- [ ] Describe how AI learns by minimizing loss\n",
    "- [ ] Choose appropriate loss functions for different tasks\n",
    "- [ ] Connect loss minimization to model improvement\n",
    "\n",
    "### Week 1 Foundation:\n",
    "- [ ] Distinguish generative vs discriminative AI\n",
    "- [ ] Understand training vs generation phases\n",
    "- [ ] Identify key GenAI components (tokenizer, neural network, sampling)\n",
    "- [ ] Explain probability's role in text generation\n",
    "- [ ] Grasp how loss functions drive learning\n",
    "\n",
    "**üéâ Week 1 Complete!** Ready for [Week 2: Tokenization Deep Dive](../../week02/)?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
