{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14205784",
   "metadata": {},
   "source": [
    "# 🎨 Day 4: Embeddings Discovery Laboratory\n",
    "\n",
    "## Educational Mission: Transforming Discrete Tokens into Semantic Intelligence\n",
    "\n",
    "Welcome to our embeddings discovery laboratory! Today we bridge the tokenization foundations from Day 3 into the realm of semantic vector representations. You'll discover how AI systems transform discrete tokens into continuous vector spaces where meaning becomes mathematically measurable.\n",
    "\n",
    "### 🎯 Learning Objectives\n",
    "\n",
    "By completing this interactive laboratory, you will:\n",
    "\n",
    "1. **Generate and Analyze Embeddings**: Create vector representations and understand their mathematical properties\n",
    "2. **Explore Semantic Similarity**: Discover how vector mathematics captures meaning relationships\n",
    "3. **Master Vector Arithmetic**: Understand how embeddings encode relational patterns\n",
    "4. **Visualize Semantic Clustering**: See how related concepts cluster in high-dimensional space\n",
    "5. **Build Semantic Search Systems**: Create practical applications using embedding intelligence\n",
    "\n",
    "### 🔄 Connection to Day 3: From Tokens to Vectors\n",
    "\n",
    "Yesterday, you discovered how tokenizers segment text into discrete units. Today, we transform those tokens into **continuous semantic vectors** that capture meaning:\n",
    "\n",
    "```\n",
    "Day 3: \"The AI model\" → [\"The\", \"AI\", \"model\"] (discrete tokens)\n",
    "Day 4: [\"The\", \"AI\", \"model\"] → [[0.2, -0.1, 0.8...], [0.9, 0.3, -0.2...], [-0.1, 0.7, 0.4...]] (semantic vectors)\n",
    "```\n",
    "\n",
    "### 🧬 What Are Embeddings?\n",
    "\n",
    "**Embeddings** are dense vector representations that capture semantic meaning in continuous space. Each dimension represents a learned semantic feature, enabling mathematical operations on meaning itself.\n",
    "\n",
    "Think of embeddings as **coordinates in meaning space** - similar concepts cluster together, while different concepts remain distant.\n",
    "\n",
    "### ⚡ Quick Setup\n",
    "\n",
    "Run this cell to import our discovery tools and set up the learning environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Discovery Laboratory Setup\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for our discovery tools\n",
    "sys.path.append('../src/day4')\n",
    "\n",
    "# Import our custom embedding discovery lab\n",
    "try:\n",
    "    from embeddings_discovery_lab import EmbeddingDiscoveryLab\n",
    "    print(\"✅ Embeddings Discovery Lab loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Discovery Lab import issue: {e}\")\n",
    "    print(\"Running in basic mode...\")\n",
    "\n",
    "# Configure display settings for educational clarity\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"🎨 Embeddings Discovery Laboratory Ready!\")\n",
    "print(\"Let's explore how AI transforms tokens into semantic intelligence...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793e9c1",
   "metadata": {},
   "source": [
    "## 🧪 Discovery Experiment 1: Basic Embedding Generation\n",
    "\n",
    "### Educational Mission: Understanding Vector Properties\n",
    "\n",
    "Let's start by generating embeddings for simple text and examining their mathematical properties. This reveals how meaning gets encoded into numerical vectors.\n",
    "\n",
    "### 🔍 What to Observe:\n",
    "- **Dimensionality**: How many features represent each concept?\n",
    "- **Value Distribution**: Are values positive, negative, or mixed?\n",
    "- **Magnitude**: How \"long\" is each vector?\n",
    "- **Sparsity**: How many dimensions are zero vs. non-zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffab836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Initialize Discovery Lab\n",
    "lab = EmbeddingDiscoveryLab()\n",
    "\n",
    "# Educational text samples for analysis\n",
    "educational_texts = [\n",
    "    \"artificial intelligence\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"natural language processing\",\n",
    "    \"deep neural networks\",\n",
    "    \"computer vision systems\"\n",
    "]\n",
    "\n",
    "print(\"🧪 DISCOVERY EXPERIMENT 1: EMBEDDING PROPERTY ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Analyze each text sample\n",
    "for i, text in enumerate(educational_texts, 1):\n",
    "    print(f\"\\n📝 Sample {i}: '{text}'\")\n",
    "\n",
    "    # Generate embedding and analyze properties\n",
    "    analysis = lab.analyze_embedding_properties(text)\n",
    "\n",
    "    if analysis:\n",
    "        print(f\"   🔢 Token Count: {analysis['token_count']}\")\n",
    "        print(f\"   📊 Vector Dimensions: {analysis['embedding_dimensions']}\")\n",
    "        print(f\"   📏 Vector Magnitude: {analysis['vector_magnitude']:.3f}\")\n",
    "        print(f\"   ⚖️ Mean Value: {analysis['mean_value']:.3f}\")\n",
    "        print(f\"   📈 Standard Deviation: {analysis['std_deviation']:.3f}\")\n",
    "        print(f\"   ➕ Positive Dimensions: {analysis['positive_dimensions']}\")\n",
    "        print(f\"   ➖ Negative Dimensions: {analysis['negative_dimensions']}\")\n",
    "        # Show first 5 tokens\n",
    "        print(f\"   🔍 Tokens: {analysis['tokens'][:5]}...\")\n",
    "\n",
    "        # Educational insights\n",
    "        if analysis['positive_dimensions'] > analysis['negative_dimensions']:\n",
    "            print(\n",
    "                f\"   💡 Insight: More positive features suggest certain semantic patterns\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"   💡 Insight: Balanced positive/negative features indicate nuanced meaning\")\n",
    "\n",
    "print(f\"\\n🎓 Key Discovery:\")\n",
    "print(f\"Each concept is represented by a high-dimensional vector where:\")\n",
    "print(f\"- Every dimension captures a learned semantic feature\")\n",
    "print(f\"- Vector magnitude relates to semantic 'strength'\")\n",
    "print(f\"- Positive/negative values encode different meaning aspects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d749bb1f",
   "metadata": {},
   "source": [
    "## 🔍 Discovery Experiment 2: Semantic Similarity Exploration\n",
    "\n",
    "### Educational Mission: Measuring Meaning Relationships\n",
    "\n",
    "Now we explore how embeddings capture semantic relationships through mathematical similarity. This is where the magic happens - similar concepts have similar vectors!\n",
    "\n",
    "### 🔍 What to Observe:\n",
    "- **Cosine Similarity**: How \"aligned\" are two meaning vectors?\n",
    "- **Semantic Patterns**: Do synonyms score higher than unrelated words?\n",
    "- **Relationship Gradients**: How do different types of relationships score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 DISCOVERY EXPERIMENT 2: SEMANTIC SIMILARITY EXPLORATION\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Educational text pairs for similarity analysis\n",
    "similarity_test_pairs = [\n",
    "    # Synonyms (should be highly similar)\n",
    "    (\"happy\", \"joyful\"),\n",
    "    (\"intelligent\", \"smart\"),\n",
    "    (\"quickly\", \"rapidly\"),\n",
    "\n",
    "    # Related concepts (moderately similar)\n",
    "    (\"computer\", \"laptop\"),\n",
    "    (\"doctor\", \"hospital\"),\n",
    "    (\"book\", \"reading\"),\n",
    "\n",
    "    # Different domains (low similarity)\n",
    "    (\"mathematics\", \"cooking\"),\n",
    "    (\"ocean\", \"software\"),\n",
    "    (\"music\", \"chemistry\"),\n",
    "\n",
    "    # Opposites (interesting case!)\n",
    "    (\"hot\", \"cold\"),\n",
    "    (\"big\", \"small\"),\n",
    "    (\"light\", \"dark\")\n",
    "]\n",
    "\n",
    "# Analyze semantic relationships\n",
    "similarity_results = lab.explore_semantic_similarity(similarity_test_pairs)\n",
    "\n",
    "# Educational analysis of results\n",
    "print(f\"\\n📊 SIMILARITY PATTERN ANALYSIS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "high_sim_pairs = []\n",
    "medium_sim_pairs = []\n",
    "low_sim_pairs = []\n",
    "\n",
    "for pair_key, result in similarity_results.items():\n",
    "    sim_score = result['cosine_similarity']\n",
    "    pair_desc = f\"{result['text1']} ↔ {result['text2']}\"\n",
    "\n",
    "    if sim_score > 0.7:\n",
    "        high_sim_pairs.append((pair_desc, sim_score))\n",
    "    elif sim_score > 0.4:\n",
    "        medium_sim_pairs.append((pair_desc, sim_score))\n",
    "    else:\n",
    "        low_sim_pairs.append((pair_desc, sim_score))\n",
    "\n",
    "print(f\"\\n🎯 High Similarity (>0.7): Strong semantic relationships\")\n",
    "for pair, score in high_sim_pairs:\n",
    "    print(f\"   {pair}: {score:.3f}\")\n",
    "\n",
    "print(f\"\\n🤔 Medium Similarity (0.4-0.7): Related concepts\")\n",
    "for pair, score in medium_sim_pairs:\n",
    "    print(f\"   {pair}: {score:.3f}\")\n",
    "\n",
    "print(f\"\\n🔄 Low Similarity (<0.4): Different semantic domains\")\n",
    "for pair, score in low_sim_pairs:\n",
    "    print(f\"   {pair}: {score:.3f}\")\n",
    "\n",
    "print(f\"\\n🎓 Key Discovery:\")\n",
    "print(f\"Embeddings capture semantic relationships through vector similarity!\")\n",
    "print(f\"- Similar meanings → similar vectors → high cosine similarity\")\n",
    "print(f\"- Different meanings → different vectors → low cosine similarity\")\n",
    "print(f\"- Even opposites can have medium similarity (they're related concepts!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e417f4",
   "metadata": {},
   "source": [
    "## 🧮 Discovery Experiment 3: Vector Arithmetic Magic\n",
    "\n",
    "### Educational Mission: Mathematics of Meaning\n",
    "\n",
    "This is where embeddings reveal their true power! We can perform arithmetic operations on meaning itself. The famous example: `King - Man + Woman = Queen`\n",
    "\n",
    "### 🔍 What to Observe:\n",
    "- **Relational Patterns**: How do embeddings capture analogies?\n",
    "- **Vector Arithmetic**: Can we mathematically manipulate meaning?\n",
    "- **Semantic Algebra**: What happens when we add/subtract concepts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908179e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧮 DISCOVERY EXPERIMENT 3: VECTOR ARITHMETIC EXPLORATION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Educational analogies for vector arithmetic discovery\n",
    "analogy_experiments = [\n",
    "    # Classic gender relationships\n",
    "    (\"king\", \"queen\", \"man\"),        # man → woman\n",
    "    (\"boy\", \"girl\", \"father\"),       # father → mother\n",
    "\n",
    "    # Geographic relationships\n",
    "    (\"Paris\", \"Rome\", \"France\"),     # France → Italy\n",
    "    (\"Tokyo\", \"Berlin\", \"Japan\"),    # Japan → Germany\n",
    "\n",
    "    # Activity relationships\n",
    "    (\"swimming\", \"skiing\", \"water\"),  # water → snow\n",
    "    (\"reading\", \"watching\", \"book\"),  # book → movie\n",
    "\n",
    "    # Professional relationships\n",
    "    (\"teacher\", \"doctor\", \"school\"),  # school → hospital\n",
    "    (\"chef\", \"programmer\", \"kitchen\")  # kitchen → computer\n",
    "]\n",
    "\n",
    "print(\"Exploring analogies: A is to B as C is to ?\")\n",
    "print(\"Vector arithmetic: B - A + C = ?\")\n",
    "\n",
    "# Discover vector arithmetic patterns\n",
    "arithmetic_results = lab.discover_vector_arithmetic(analogy_experiments)\n",
    "\n",
    "# Educational analysis of vector arithmetic\n",
    "print(f\"\\n📊 VECTOR ARITHMETIC PATTERN ANALYSIS\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "successful_analogies = []\n",
    "partial_analogies = []\n",
    "\n",
    "for analogy_key, result in arithmetic_results.items():\n",
    "    similarity = result['similarity_score']\n",
    "    relationship = result['relationship']\n",
    "\n",
    "    if similarity > 0.6:\n",
    "        successful_analogies.append((relationship, similarity))\n",
    "    else:\n",
    "        partial_analogies.append((relationship, similarity))\n",
    "\n",
    "print(f\"\\n🎯 Strong Analogies (>0.6): Clear relational patterns\")\n",
    "for relationship, score in successful_analogies:\n",
    "    print(f\"   {relationship} (similarity: {score:.3f})\")\n",
    "\n",
    "print(f\"\\n🤔 Partial Analogies (≤0.6): Weaker but still meaningful\")\n",
    "for relationship, score in partial_analogies:\n",
    "    print(f\"   {relationship} (similarity: {score:.3f})\")\n",
    "\n",
    "print(f\"\\n🎓 Key Discovery:\")\n",
    "print(f\"Vector arithmetic reveals the algebraic structure of meaning!\")\n",
    "print(f\"- Embeddings encode relational patterns as vector differences\")\n",
    "print(f\"- We can manipulate concepts mathematically: Queen = King - Man + Woman\")\n",
    "print(f\"- Semantic relationships become geometric transformations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f0199",
   "metadata": {},
   "source": [
    "## 🎨 Discovery Experiment 4: Semantic Clustering Visualization\n",
    "\n",
    "### Educational Mission: Visualizing Meaning Space\n",
    "\n",
    "Let's visualize how embeddings organize concepts in semantic space. Related concepts should cluster together, while different domains remain separate.\n",
    "\n",
    "### 🔍 What to Observe:\n",
    "- **Semantic Clusters**: Do related concepts group together?\n",
    "- **Boundary Formation**: Where do different semantic domains separate?\n",
    "- **Dimensional Reduction**: How does 1536D → 2D preserve relationships?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎨 DISCOVERY EXPERIMENT 4: SEMANTIC CLUSTERING VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Educational word groups for clustering analysis\n",
    "semantic_word_groups = {\n",
    "    \"AI_Technology\": [\n",
    "        \"artificial intelligence\",\n",
    "        \"machine learning\",\n",
    "        \"neural networks\",\n",
    "        \"deep learning\",\n",
    "        \"computer vision\"\n",
    "    ],\n",
    "    \"Programming\": [\n",
    "        \"python\",\n",
    "        \"javascript\",\n",
    "        \"programming\",\n",
    "        \"software\",\n",
    "        \"algorithm\"\n",
    "    ],\n",
    "    \"Science\": [\n",
    "        \"physics\",\n",
    "        \"chemistry\",\n",
    "        \"biology\",\n",
    "        \"mathematics\",\n",
    "        \"research\"\n",
    "    ],\n",
    "    \"Nature\": [\n",
    "        \"forest\",\n",
    "        \"ocean\",\n",
    "        \"mountain\",\n",
    "        \"wildlife\",\n",
    "        \"ecosystem\"\n",
    "    ],\n",
    "    \"Arts\": [\n",
    "        \"painting\",\n",
    "        \"music\",\n",
    "        \"literature\",\n",
    "        \"sculpture\",\n",
    "        \"creativity\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Analyzing semantic clustering patterns...\")\n",
    "print(f\"Word groups: {list(semantic_word_groups.keys())}\")\n",
    "print(\n",
    "    f\"Total concepts: {sum(len(words) for words in semantic_word_groups.values())}\")\n",
    "\n",
    "# Visualize semantic clustering\n",
    "clustering_results = lab.visualize_semantic_clusters(semantic_word_groups)\n",
    "\n",
    "# Educational analysis of clustering\n",
    "if clustering_results:\n",
    "    print(f\"\\n📊 CLUSTERING ANALYSIS RESULTS\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    total_variance = sum(clustering_results['pca_explained_variance'])\n",
    "    print(f\"\\n🎯 Dimensionality Reduction Quality:\")\n",
    "    print(f\"   Total variance preserved: {total_variance:.1%}\")\n",
    "    print(\n",
    "        f\"   PC1 explains: {clustering_results['pca_explained_variance'][0]:.1%}\")\n",
    "    print(\n",
    "        f\"   PC2 explains: {clustering_results['pca_explained_variance'][1]:.1%}\")\n",
    "\n",
    "    print(f\"\\n🔍 Cluster Assignments:\")\n",
    "    cluster_groups = {}\n",
    "    for word, cluster in clustering_results['cluster_assignments'].items():\n",
    "        if cluster not in cluster_groups:\n",
    "            cluster_groups[cluster] = []\n",
    "        cluster_groups[cluster].append(word)\n",
    "\n",
    "    for cluster_id, words in cluster_groups.items():\n",
    "        print(f\"   Cluster {cluster_id}: {', '.join(words[:3])}...\")\n",
    "\n",
    "print(f\"\\n🎓 Key Discovery:\")\n",
    "print(f\"Embeddings naturally organize concepts into semantic neighborhoods!\")\n",
    "print(f\"- Related concepts cluster together in high-dimensional space\")\n",
    "print(f\"- Different semantic domains form distinct regions\")\n",
    "print(f\"- 2D visualization preserves meaningful relationships despite compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4daa9",
   "metadata": {},
   "source": [
    "## 🔍 Discovery Experiment 5: Contextual Embedding Analysis\n",
    "\n",
    "### Educational Mission: Understanding Context Dependence\n",
    "\n",
    "One fascinating property of modern embeddings is their **context sensitivity**. The same word can have different embeddings depending on its context!\n",
    "\n",
    "### 🔍 What to Observe:\n",
    "- **Context Effects**: How does surrounding text change word meaning?\n",
    "- **Polysemy**: How are multiple word meanings handled?\n",
    "- **Semantic Disambiguation**: Can embeddings distinguish word senses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 DISCOVERY EXPERIMENT 5: CONTEXTUAL EMBEDDING ANALYSIS\")\n",
    "print(\"=\" * 57)\n",
    "\n",
    "# Educational examples of context-dependent meaning\n",
    "contextual_examples = [\n",
    "    # \"Bank\" in different contexts\n",
    "    {\n",
    "        \"word\": \"bank\",\n",
    "        \"contexts\": [\n",
    "            \"I deposited money at the bank\",\n",
    "            \"We sat by the river bank\",\n",
    "            \"The airplane made a sharp bank to the left\"\n",
    "        ]\n",
    "    },\n",
    "    # \"Apple\" in different contexts\n",
    "    {\n",
    "        \"word\": \"apple\",\n",
    "        \"contexts\": [\n",
    "            \"I ate a red apple for lunch\",\n",
    "            \"Apple released a new iPhone\",\n",
    "            \"The apple tree in our garden\"\n",
    "        ]\n",
    "    },\n",
    "    # \"Light\" in different contexts\n",
    "    {\n",
    "        \"word\": \"light\",\n",
    "        \"contexts\": [\n",
    "            \"Turn on the light switch\",\n",
    "            \"The box was surprisingly light\",\n",
    "            \"Light travels at incredible speed\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Analyzing how context influences embedding meaning...\\n\")\n",
    "\n",
    "for example in contextual_examples:\n",
    "    word = example[\"word\"]\n",
    "    contexts = example[\"contexts\"]\n",
    "\n",
    "    print(f\"📝 Analyzing '{word}' in different contexts:\")\n",
    "\n",
    "    # Get embeddings for each context\n",
    "    context_embeddings = []\n",
    "    for i, context in enumerate(contexts, 1):\n",
    "        embedding = lab.get_embedding(context)\n",
    "        if embedding is not None:\n",
    "            context_embeddings.append(embedding)\n",
    "            print(f\"   Context {i}: '{context}'\")\n",
    "\n",
    "    # Analyze similarities between different contexts\n",
    "    if len(context_embeddings) >= 2:\n",
    "        print(f\"\\n   🔍 Contextual Similarity Analysis:\")\n",
    "\n",
    "        for i in range(len(context_embeddings)):\n",
    "            for j in range(i + 1, len(context_embeddings)):\n",
    "                similarity = cosine_similarity([context_embeddings[i]], [\n",
    "                                               context_embeddings[j]])[0][0]\n",
    "                print(f\"      Context {i+1} ↔ Context {j+1}: {similarity:.3f}\")\n",
    "\n",
    "                # Educational insights\n",
    "                if similarity > 0.8:\n",
    "                    print(f\"         💡 High similarity - similar meaning/usage\")\n",
    "                elif similarity > 0.6:\n",
    "                    print(f\"         🤔 Moderate similarity - related but distinct\")\n",
    "                else:\n",
    "                    print(f\"         🔄 Low similarity - different semantic domains\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(f\"🎓 Key Discovery:\")\n",
    "print(f\"Modern embeddings are context-aware and capture meaning nuances!\")\n",
    "print(f\"- Same word → different embeddings → different meanings\")\n",
    "print(f\"- Context provides disambiguation for polysemous words\")\n",
    "print(f\"- Embeddings encode both word identity and contextual usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c9ea8",
   "metadata": {},
   "source": [
    "## 🎯 Discovery Experiment 6: Advanced Similarity Search\n",
    "\n",
    "### Educational Mission: Building Semantic Search Systems\n",
    "\n",
    "Now let's build a practical application! We'll create a semantic search system that finds relevant documents based on meaning rather than keyword matching.\n",
    "\n",
    "### 🔍 What to Observe:\n",
    "- **Semantic vs. Keyword Search**: How do results differ?\n",
    "- **Relevance Ranking**: How are documents ordered by semantic similarity?\n",
    "- **Query Understanding**: How does the system interpret search intent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94756f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 DISCOVERY EXPERIMENT 6: ADVANCED SIMILARITY SEARCH\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "# Educational document collection for semantic search\n",
    "document_collection = [\n",
    "    \"Machine learning algorithms can identify patterns in large datasets automatically.\",\n",
    "    \"Deep neural networks consist of multiple layers that process information hierarchically.\",\n",
    "    \"Natural language processing enables computers to understand and generate human language.\",\n",
    "    \"Computer vision systems can recognize objects and scenes in digital images.\",\n",
    "    \"Reinforcement learning agents learn optimal behavior through trial and error.\",\n",
    "    \"Transformers revolutionized language modeling with attention mechanisms.\",\n",
    "    \"Convolutional networks excel at processing grid-like data such as images.\",\n",
    "    \"Recurrent networks can model sequential data with temporal dependencies.\",\n",
    "    \"Generative models create new data samples that resemble training data.\",\n",
    "    \"Supervised learning requires labeled examples to train predictive models.\",\n",
    "    \"Unsupervised learning discovers hidden structures in unlabeled data.\",\n",
    "    \"Transfer learning adapts pre-trained models to new domains efficiently.\"\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Building semantic search system with {len(document_collection)} documents...\\n\")\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "print(\"📊 Generating document embeddings...\")\n",
    "document_embeddings = []\n",
    "for i, doc in enumerate(document_collection):\n",
    "    embedding = lab.get_embedding(doc)\n",
    "    if embedding is not None:\n",
    "        document_embeddings.append(embedding)\n",
    "        print(f\"   Document {i+1}: Embedded ({len(embedding)} dimensions)\")\n",
    "\n",
    "# Test semantic search with different queries\n",
    "search_queries = [\n",
    "    \"How do neural networks learn from data?\",\n",
    "    \"What techniques work best for image recognition?\",\n",
    "    \"How can AI understand human speech?\",\n",
    "    \"What approaches don't require labeled training data?\"\n",
    "]\n",
    "\n",
    "print(f\"\\n🔍 SEMANTIC SEARCH EXPERIMENTS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for query_num, query in enumerate(search_queries, 1):\n",
    "    print(f\"\\n🎯 Query {query_num}: '{query}'\")\n",
    "\n",
    "    # Generate query embedding\n",
    "    query_embedding = lab.get_embedding(query)\n",
    "    if query_embedding is None:\n",
    "        continue\n",
    "\n",
    "    # Calculate similarities to all documents\n",
    "    similarities = []\n",
    "    for doc_embedding in document_embeddings:\n",
    "        similarity = cosine_similarity(\n",
    "            [query_embedding], [doc_embedding])[0][0]\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Rank documents by similarity\n",
    "    ranked_docs = sorted(enumerate(similarities),\n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\n   📋 Top 3 Most Relevant Documents:\")\n",
    "    for rank, (doc_idx, similarity) in enumerate(ranked_docs[:3], 1):\n",
    "        doc_text = document_collection[doc_idx][:60] + \"...\"\n",
    "        print(f\"      {rank}. [{similarity:.3f}] {doc_text}\")\n",
    "\n",
    "        # Educational insights about relevance\n",
    "        if similarity > 0.7:\n",
    "            print(f\"         💡 Highly relevant - strong semantic match\")\n",
    "        elif similarity > 0.5:\n",
    "            print(f\"         🤔 Moderately relevant - partial semantic overlap\")\n",
    "        else:\n",
    "            print(f\"         📊 Weakly relevant - limited semantic connection\")\n",
    "\n",
    "print(f\"\\n🎓 Key Discovery:\")\n",
    "print(f\"Semantic search understands meaning beyond keywords!\")\n",
    "print(f\"- Queries match documents by conceptual similarity\")\n",
    "print(f\"- Results ranked by semantic relevance, not keyword frequency\")\n",
    "print(f\"- System understands intent even with different vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7809633",
   "metadata": {},
   "source": [
    "## 🎓 Day 4 Discovery Summary\n",
    "\n",
    "### 🎯 What You've Discovered\n",
    "\n",
    "Congratulations! You've completed a comprehensive exploration of embeddings and their remarkable properties. Here's what you've mastered:\n",
    "\n",
    "#### 🧬 **Embedding Fundamentals**\n",
    "- **Vector Representation**: How discrete tokens become continuous semantic vectors\n",
    "- **Mathematical Properties**: Dimensionality, magnitude, and distribution patterns\n",
    "- **Semantic Encoding**: How meaning gets captured in numerical form\n",
    "\n",
    "#### 🔍 **Similarity and Relationships** \n",
    "- **Cosine Similarity**: Mathematical measurement of semantic closeness\n",
    "- **Relationship Patterns**: How synonyms, related concepts, and opposites compare\n",
    "- **Context Sensitivity**: How surrounding text influences meaning representation\n",
    "\n",
    "#### 🧮 **Vector Arithmetic Magic**\n",
    "- **Semantic Algebra**: Performing arithmetic operations on meaning itself\n",
    "- **Analogy Discovery**: Using vector math to find relational patterns\n",
    "- **Mathematical Meaning**: Understanding embeddings as algebraic structures\n",
    "\n",
    "#### 🎨 **Visualization and Clustering**\n",
    "- **Semantic Space**: How concepts organize in high-dimensional space\n",
    "- **Clustering Patterns**: Natural grouping of related concepts\n",
    "- **Dimensionality Reduction**: Preserving relationships in lower dimensions\n",
    "\n",
    "#### 🎯 **Practical Applications**\n",
    "- **Semantic Search**: Building meaning-based information retrieval\n",
    "- **Relevance Ranking**: Ordering results by conceptual similarity\n",
    "- **Real-world Systems**: Understanding how modern AI applications work\n",
    "\n",
    "### 🔄 Connection to Tomorrow (Day 5)\n",
    "\n",
    "Today's embedding discoveries set the foundation for tomorrow's attention mechanisms exploration:\n",
    "\n",
    "```\n",
    "Day 4: \"king\" → [0.2, -0.1, 0.8, ...] (semantic vector)\n",
    "Day 5: How does \"king\" attend to \"crown\", \"royal\", \"kingdom\"? (attention patterns)\n",
    "```\n",
    "\n",
    "**Tomorrow's Preview**: Attention mechanisms use embedding similarities to determine which words should \"pay attention\" to each other when processing sequences!\n",
    "\n",
    "### 🚀 Keep Exploring\n",
    "\n",
    "Try these follow-up experiments:\n",
    "\n",
    "1. **Custom Vocabulary**: Test embeddings with domain-specific terms from your field\n",
    "2. **Language Comparison**: Compare embeddings for the same concept in different languages\n",
    "3. **Temporal Analysis**: Explore how word meanings change over time\n",
    "4. **Multimodal Embeddings**: Investigate image-text embedding combinations\n",
    "\n",
    "### 💡 Key Insights to Remember\n",
    "\n",
    "- **Embeddings Transform Discrete → Continuous**: Converting tokens into measurable meaning\n",
    "- **Mathematics Captures Semantics**: Vector operations reveal semantic relationships\n",
    "- **Context Matters**: Modern embeddings adapt meaning based on surrounding text\n",
    "- **Practical Power**: Semantic search demonstrates real-world embedding applications\n",
    "\n",
    "You've now mastered the foundational technology that powers modern AI language understanding! 🎉"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
